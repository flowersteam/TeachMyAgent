<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>TeachMyAgent.students.spinup.algos.tf1.sac_v011.sac API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>TeachMyAgent.students.spinup.algos.tf1.sac_v011.sac</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
import tensorflow as tf
import time
from TeachMyAgent.students.spinup.algos.tf1.sac_v011 import core
from TeachMyAgent.students.spinup.algos.tf1.sac_v011.core import get_vars
from TeachMyAgent.students.spinup.utils.logx import EpochLogger

class ReplayBuffer:
    &#34;&#34;&#34;
    A simple FIFO experience replay buffer for SAC agents.
    &#34;&#34;&#34;

    def __init__(self, obs_dim, act_dim, size):
        self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)
        self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)
        self.acts_buf = np.zeros([size, act_dim], dtype=np.float32)
        self.rews_buf = np.zeros(size, dtype=np.float32)
        self.done_buf = np.zeros(size, dtype=np.float32)
        self.ptr, self.size, self.max_size = 0, 0, size

    def store(self, obs, act, rew, next_obs, done):
        self.obs1_buf[self.ptr] = obs
        self.obs2_buf[self.ptr] = next_obs
        self.acts_buf[self.ptr] = act
        self.rews_buf[self.ptr] = rew
        self.done_buf[self.ptr] = done
        self.ptr = (self.ptr+1) % self.max_size
        self.size = min(self.size+1, self.max_size)

    def sample_batch(self, batch_size=32):
        idxs = np.random.randint(0, self.size, size=batch_size)
        return dict(obs1=self.obs1_buf[idxs],
                    obs2=self.obs2_buf[idxs],
                    acts=self.acts_buf[idxs],
                    rews=self.rews_buf[idxs],
                    done=self.done_buf[idxs])

&#34;&#34;&#34;

Soft Actor-Critic

(With slight variations that bring it closer to TD3)
&#34;&#34;&#34;
def sac(env_fn, actor_critic=core.mlp_actor_critic, ac_kwargs=dict(), seed=0,
        steps_per_epoch=4000, epochs=100, replay_size=2000000, gamma=0.99,
        polyak=0.995, lr=1e-3, alpha=0.2, batch_size=100, start_steps=10000,
        max_ep_len=2000, logger_kwargs=dict(), save_freq=1,
        nb_test_episodes=50, train_freq=10, Teacher=None, half_save=False,
        pretrained_model=None, reset_frequency=None):
    &#34;&#34;&#34;
    Soft Actor-Critic

    With some modifications were made to make it use an ACL teacher and a test env.

    Args:
        env_fn : A function which creates a copy of the environment.
            The environment must satisfy the OpenAI Gym API.

        actor_critic: A function which takes in placeholder symbols 
            for state, ``x_ph``, and action, ``a_ph``, and returns the main 
            outputs from the agent&#39;s Tensorflow computation graph:

            ===========  ================  ======================================
            Symbol       Shape             Description
            ===========  ================  ======================================
            ``mu``       (batch, act_dim)  | Computes mean actions from policy
                                           | given states.
            ``pi``       (batch, act_dim)  | Samples actions from policy given 
                                           | states.
            ``logp_pi``  (batch,)          | Gives log probability, according to
                                           | the policy, of the action sampled by
                                           | ``pi``. Critical: must be differentiable
                                           | with respect to policy parameters all
                                           | the way through action sampling.
            ``q1``       (batch,)          | Gives one estimate of Q* for 
                                           | states in ``x_ph`` and actions in
                                           | ``a_ph``.
            ``q2``       (batch,)          | Gives another estimate of Q* for 
                                           | states in ``x_ph`` and actions in
                                           | ``a_ph``.
            ``q1_pi``    (batch,)          | Gives the composition of ``q1`` and 
                                           | ``pi`` for states in ``x_ph``: 
                                           | q1(x, pi(x)).
            ``q2_pi``    (batch,)          | Gives the composition of ``q2`` and 
                                           | ``pi`` for states in ``x_ph``: 
                                           | q2(x, pi(x)).
            ``v``        (batch,)          | Gives the value estimate for states
                                           | in ``x_ph``. 
            ===========  ================  ======================================

        ac_kwargs (dict): Any kwargs appropriate for the actor_critic 
            function you provided to SAC.

        seed (int): Seed for random number generators.

        steps_per_epoch (int): Number of steps of interaction (state-action pairs) 
            for the agent and the environment in each epoch.

        epochs (int): Number of epochs to run and train agent.

        replay_size (int): Maximum length of replay buffer.

        gamma (float): Discount factor. (Always between 0 and 1.)

        polyak (float): Interpolation factor in polyak averaging for target 
            networks. Target networks are updated towards main networks 
            according to:

            .. math:: \\theta_{\\text{targ}} \\leftarrow 
                \\rho \\theta_{\\text{targ}} + (1-\\rho) \\theta

            where :math:`\\rho` is polyak. (Always between 0 and 1, usually 
            close to 1.)

        lr (float): Learning rate (used for both policy and value learning).

        alpha (float): Entropy regularization coefficient. (Equivalent to 
            inverse of reward scale in the original SAC paper.)

        batch_size (int): Minibatch size for SGD.

        start_steps (int): Number of steps for uniform-random action selection,
            before running real policy. Helps exploration.

        max_ep_len (int): Maximum length of trajectory / episode / rollout.

        logger_kwargs (dict): Keyword args for EpochLogger.

        save_freq (int): How often (in terms of gap between epochs) to save
            the current policy and value function.

    &#34;&#34;&#34;


    logger = EpochLogger(**logger_kwargs)
    hyperparams = locals().copy()

    if Teacher:
        del hyperparams[&#39;Teacher&#39;]  # remove teacher to avoid serialization error
        # del hyperparams[&#39;replay_buffer&#39;]
    logger.save_config(hyperparams)

    tf.set_random_seed(seed)
    np.random.seed(seed)

    env, test_env = env_fn(), env_fn()

    if Teacher:
        params = Teacher.set_env_params(env)
    env.reset()

    if not hasattr(env, &#34;env&#34;):
        obs_space = env.observation_space
        act_space = env.action_space
    else:
        obs_space = env.env.observation_space
        act_space = env.env.action_space

    obs_dim = obs_space.shape[0]
    act_dim = act_space.shape[0]

    # Action limit for clamping: critically, assumes all dimensions share the same bound!
    act_limit = act_space.high[0]

    # Share information about action space with policy architecture
    ac_kwargs[&#39;action_space&#39;] = act_space

    # Inputs to computation graph
    x_ph, a_ph, x2_ph, r_ph, d_ph = core.placeholders(obs_dim, act_dim, obs_dim, None, None)

    # Main outputs from computation graph
    with tf.variable_scope(&#39;main&#39;):
        mu, pi, logp_pi, q1, q2, q1_pi, q2_pi, v = actor_critic(x_ph, a_ph, **ac_kwargs)
    
    # Target value network
    with tf.variable_scope(&#39;target&#39;):
        _, _, _, _, _, _, _, v_targ  = actor_critic(x2_ph, a_ph, **ac_kwargs)

    # start Experience buffer (only usefull for episodic replay buffer)
    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)
    # replay_buffer.start_task(params)

    # Count variables
    var_counts = tuple(core.count_vars(scope) for scope in 
                       [&#39;main/pi&#39;, &#39;main/q1&#39;, &#39;main/q2&#39;, &#39;main/v&#39;, &#39;main&#39;])
    print((&#39;\nNumber of parameters: \t pi: %d, \t&#39; + \
           &#39;q1: %d, \t q2: %d, \t v: %d, \t total: %d\n&#39;)%var_counts)

    # Min Double-Q:
    min_q_pi = tf.minimum(q1_pi, q2_pi)

    # Targets for Q and V regression
    q_backup = tf.stop_gradient(r_ph + gamma*(1-d_ph)*v_targ)
    v_backup = tf.stop_gradient(min_q_pi - alpha * logp_pi)

    # Soft actor-critic losses
    pi_loss = tf.reduce_mean(alpha * logp_pi - q1_pi)
    q1_loss = 0.5 * tf.reduce_mean((q_backup - q1)**2)
    q2_loss = 0.5 * tf.reduce_mean((q_backup - q2)**2)
    v_loss = 0.5 * tf.reduce_mean((v_backup - v)**2)
    value_loss = q1_loss + q2_loss + v_loss

    # Policy train op 
    # (has to be separate from value train op, because q1_pi appears in pi_loss)
    pi_optimizer = tf.train.AdamOptimizer(learning_rate=lr)
    train_pi_op = pi_optimizer.minimize(pi_loss, var_list=get_vars(&#39;main/pi&#39;))

    # Value train op
    # (control dep of train_pi_op because sess.run otherwise evaluates in nondeterministic order)
    value_optimizer = tf.train.AdamOptimizer(learning_rate=lr)
    value_params = get_vars(&#39;main/q&#39;) + get_vars(&#39;main/v&#39;)
    with tf.control_dependencies([train_pi_op]):
        train_value_op = value_optimizer.minimize(value_loss, var_list=value_params)

    # Polyak averaging for target variables
    # (control flow because sess.run otherwise evaluates in nondeterministic order)
    with tf.control_dependencies([train_value_op]):
        target_update = tf.group([tf.assign(v_targ, polyak*v_targ + (1-polyak)*v_main)
                                  for v_main, v_targ in zip(get_vars(&#39;main&#39;), get_vars(&#39;target&#39;))])

    # All ops to call during one training step
    step_ops = [pi_loss, q1_loss, q2_loss, v_loss, q1, q2, v, logp_pi, 
                train_pi_op, train_value_op, target_update]

    # Initializing targets to match main variables
    target_init = tf.group([tf.assign(v_targ, v_main)
                              for v_main, v_targ in zip(get_vars(&#39;main&#39;), get_vars(&#39;target&#39;))])

    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True

    if pretrained_model is not None: # set checkpoint weights
        saver = tf.train.Saver()

    sess = tf.Session(config=config)

    if pretrained_model is not None:  # set checkpoint weights
        print(&#34;restoring trained weights&#34;)
        saver.restore(sess, pretrained_model)
        print(&#39;restored&#39;)
    else:
        sess.run(tf.global_variables_initializer())
        sess.run(target_init)

    # Setup model saving
    logger.setup_tf_saver(sess, inputs={&#39;x&#39;: x_ph, &#39;a&#39;: a_ph}, 
                                outputs={&#39;mu&#39;: mu, &#39;pi&#39;: pi, &#39;q1&#39;: q1, &#39;q2&#39;: q2, &#39;v&#39;: v})

    value_estimator_fn = lambda states: np.squeeze(sess.run(v, feed_dict={x_ph: states}))
    Teacher.set_value_estimator(value_estimator_fn)

    def get_action(o, deterministic=False):
        act_op = mu if deterministic else pi
        return sess.run(act_op, feed_dict={x_ph: o.reshape(1,-1)})[0]

    def test_agent(n=10):
        global sess, mu, pi, q1, q2, q1_pi, q2_pi
        for j in range(n):
            if Teacher:
                test_params = Teacher.set_test_env_params(test_env)
            else:
                test_params = None

            try:
                o, r, d, ep_ret, ep_len = test_env.reset(), 0, False, 0, 0
                while not(d or (ep_len == max_ep_len)):
                    # Take deterministic actions at test time
                    o, r, d, _ = test_env.step(get_action(o, True))
                    ep_ret += r
                    ep_len += 1
                logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)
                if Teacher: Teacher.record_test_episode(ep_ret, ep_len)
            except:
                print(&#34;### TEST ERROR ###&#34;)
                print(&#34;Env params: {}&#34;.format(test_params))
                print(&#34;Test nb: {}&#34;.format(j))
                if Teacher:
                    Teacher.dump(logger.output_dir + &#39;env_params_save.pkl&#39;)
                logger.save_state({&#39;env&#39;: env}, None)
                raise

    start_time = time.time()
    o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0
    Teacher.record_train_task_initial_state(o)
    total_steps = steps_per_epoch * epochs

    # Main loop: collect experience in env and update/log each epoch
    for t in range(total_steps):
        &#34;&#34;&#34;
        Until start_steps have elapsed, randomly sample actions
        from a uniform distribution for better exploration. Afterwards, 
        use the learned policy. 
        &#34;&#34;&#34;
        if t &gt; start_steps:
            a = get_action(o)
        else:
            a = act_space.sample()

        # Step the env

        o2, r, d, infos = env.step(a)
        ep_ret += r
        ep_len += 1
        Teacher.record_train_step(o, a, r, o2, d)

        # Ignore the &#34;done&#34; signal if it comes from hitting the time
        # horizon (that is, when it&#39;s an artificial terminal signal
        # that isn&#39;t based on the agent&#39;s state)
        d = False if ep_len==max_ep_len else d

        # Store experience to replay buffer
        replay_buffer.store(o, a, r, o2, d)

        # Super critical, easy to overlook step: make sure to update 
        # most recent observation!
        o = o2

        if d or (ep_len == max_ep_len):
            # replay_buffer.end_task(ep_ret, ep_len)
            &#34;&#34;&#34;
            Perform all SAC updates at the end of the trajectory.
            This is a slight difference from the SAC specified in the
            original paper.
            &#34;&#34;&#34;
            for j in range(np.ceil(ep_len/train_freq).astype(&#39;int&#39;)):
                batch = replay_buffer.sample_batch(batch_size)
                feed_dict = {x_ph: batch[&#39;obs1&#39;],
                             x2_ph: batch[&#39;obs2&#39;],
                             a_ph: batch[&#39;acts&#39;],
                             r_ph: batch[&#39;rews&#39;],
                             d_ph: batch[&#39;done&#39;],
                            }
                outs = sess.run(step_ops, feed_dict)
                # logger.store(LossPi=outs[0], LossQ1=outs[1], LossQ2=outs[2],
                #              LossV=outs[3], Q1Vals=outs[4], Q2Vals=outs[5],
                #              VVals=outs[6], LogPi=outs[7])
            logger.store(EpRet=ep_ret, EpLen=ep_len)
            if Teacher:
                success = False if &#39;success&#39; not in infos else infos[&#34;success&#34;]
                Teacher.record_train_episode(ep_ret, ep_len, success)
                params = Teacher.set_env_params(env)
                # replay_buffer.start_task(params)
            o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0
            Teacher.record_train_task_initial_state(o)

        # End of epoch wrap-up
        if t &gt; 0 and (t + 1) % steps_per_epoch == 0:
            epoch = (t + 1) // steps_per_epoch

            # Save model
            if epoch % save_freq == 0:  # or (epoch == epochs):
                if half_save and epoch == epochs/2:
                    logger.save_state({&#39;env&#39;: env, &#39;test_env&#39;: test_env}, None)#itr=epoch)
                else:
                    logger.save_state({&#39;env&#39;: env, &#39;test_env&#39;: test_env}, None)#itr=epoch)

            # Test the performance of the deterministic version of the agent.
            test_agent(n=nb_test_episodes)
            # Log info about epoch
            logger.log_tabular(&#39;Epoch&#39;, epoch)
            logger.log_tabular(&#39;EpRet&#39;, with_min_and_max=True)
            logger.log_tabular(&#39;TestEpRet&#39;, with_min_and_max=True)
            logger.log_tabular(&#39;EpLen&#39;, average_only=True)
            logger.log_tabular(&#39;TestEpLen&#39;, average_only=True)
            logger.log_tabular(&#39;TotalEnvInteracts&#39;, t+1)
            #logger.log_tabular(&#39;Q1Vals&#39;, with_min_and_max=True)
            #logger.log_tabular(&#39;Q2Vals&#39;, with_min_and_max=True)
            #logger.log_tabular(&#39;VVals&#39;, with_min_and_max=True)
            #logger.log_tabular(&#39;LogPi&#39;, with_min_and_max=True)
            #logger.log_tabular(&#39;LossPi&#39;, average_only=True)
            #logger.log_tabular(&#39;LossQ1&#39;, average_only=True)
            #logger.log_tabular(&#39;LossQ2&#39;, average_only=True)
            #logger.log_tabular(&#39;LossV&#39;, average_only=True)
            logger.log_tabular(&#39;Time&#39;, time.time()-start_time)
            logger.dump_tabular()

            # Pickle parameterized env data
            #print(logger.output_dir+&#39;/env_params_save.pkl&#39;)
            if Teacher: Teacher.dump(logger.output_dir+&#39;/env_params_save.pkl&#39;)

        #### RESET ####
        if reset_frequency is not None and t &gt; 0 and t % reset_frequency == 0:
            print(&#34;Reset student.&#34;)
            sess.run(tf.global_variables_initializer())
            sess.run(target_init)
            replay_buffer.ptr, replay_buffer.size = 0, 0  # reset replay buffer</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="TeachMyAgent.students.spinup.algos.tf1.sac_v011.sac.sac"><code class="name flex">
<span>def <span class="ident">sac</span></span>(<span>env_fn, actor_critic=&lt;function mlp_actor_critic&gt;, ac_kwargs={}, seed=0, steps_per_epoch=4000, epochs=100, replay_size=2000000, gamma=0.99, polyak=0.995, lr=0.001, alpha=0.2, batch_size=100, start_steps=10000, max_ep_len=2000, logger_kwargs={}, save_freq=1, nb_test_episodes=50, train_freq=10, Teacher=None, half_save=False, pretrained_model=None, reset_frequency=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Soft Actor-Critic</p>
<p>With some modifications were made to make it use an ACL teacher and a test env.</p>
<h2 id="args">Args</h2>
<p>env_fn : A function which creates a copy of the environment.
The environment must satisfy the OpenAI Gym API.</p>
<dl>
<dt><strong><code>actor_critic</code></strong></dt>
<dd>
<p>A function which takes in placeholder symbols
for state, <code>x_ph</code>, and action, <code>a_ph</code>, and returns the main
outputs from the agent's Tensorflow computation graph:</p>
<p>===========
================
======================================
Symbol
Shape
Description
===========
================
======================================
<code>mu</code>
(batch, act_dim)
| Computes mean actions from policy
| given states.
<code>pi</code>
(batch, act_dim)
| Samples actions from policy given
| states.
<code>logp_pi</code>
(batch,)
| Gives log probability, according to
| the policy, of the action sampled by
| <code>pi</code>. Critical: must be differentiable
| with respect to policy parameters all
| the way through action sampling.
<code>q1</code>
(batch,)
| Gives one estimate of Q<em> for
| states in <code>x_ph</code> and actions in
| <code>a_ph</code>.
<code>q2</code>
(batch,)
| Gives another estimate of Q</em> for
| states in <code>x_ph</code> and actions in
| <code>a_ph</code>.
<code>q1_pi</code>
(batch,)
| Gives the composition of <code>q1</code> and
| <code>pi</code> for states in <code>x_ph</code>:
| q1(x, pi(x)).
<code>q2_pi</code>
(batch,)
| Gives the composition of <code>q2</code> and
| <code>pi</code> for states in <code>x_ph</code>:
| q2(x, pi(x)).
<code>v</code>
(batch,)
| Gives the value estimate for states
| in <code>x_ph</code>.
===========
================
======================================</p>
</dd>
<dt><strong><code>ac_kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Any kwargs appropriate for the actor_critic
function you provided to SAC.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Seed for random number generators.</dd>
<dt><strong><code>steps_per_epoch</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of steps of interaction (state-action pairs)
for the agent and the environment in each epoch.</dd>
<dt><strong><code>epochs</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of epochs to run and train agent.</dd>
<dt><strong><code>replay_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum length of replay buffer.</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code></dt>
<dd>Discount factor. (Always between 0 and 1.)</dd>
<dt><strong><code>polyak</code></strong> :&ensp;<code>float</code></dt>
<dd>
<p>Interpolation factor in polyak averaging for target
networks. Target networks are updated towards main networks
according to:</p>
<p>[ \rho \theta_{\text{targ}} + (1-\rho) \theta ]
where :math:<code>\rho</code> is polyak. (Always between 0 and 1, usually
close to 1.)</p>
</dd>
<dt><strong><code>lr</code></strong> :&ensp;<code>float</code></dt>
<dd>Learning rate (used for both policy and value learning).</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>Entropy regularization coefficient. (Equivalent to
inverse of reward scale in the original SAC paper.)</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Minibatch size for SGD.</dd>
<dt><strong><code>start_steps</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of steps for uniform-random action selection,
before running real policy. Helps exploration.</dd>
<dt><strong><code>max_ep_len</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum length of trajectory / episode / rollout.</dd>
<dt><strong><code>logger_kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Keyword args for EpochLogger.</dd>
<dt><strong><code>save_freq</code></strong> :&ensp;<code>int</code></dt>
<dd>How often (in terms of gap between epochs) to save
the current policy and value function.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sac(env_fn, actor_critic=core.mlp_actor_critic, ac_kwargs=dict(), seed=0,
        steps_per_epoch=4000, epochs=100, replay_size=2000000, gamma=0.99,
        polyak=0.995, lr=1e-3, alpha=0.2, batch_size=100, start_steps=10000,
        max_ep_len=2000, logger_kwargs=dict(), save_freq=1,
        nb_test_episodes=50, train_freq=10, Teacher=None, half_save=False,
        pretrained_model=None, reset_frequency=None):
    &#34;&#34;&#34;
    Soft Actor-Critic

    With some modifications were made to make it use an ACL teacher and a test env.

    Args:
        env_fn : A function which creates a copy of the environment.
            The environment must satisfy the OpenAI Gym API.

        actor_critic: A function which takes in placeholder symbols 
            for state, ``x_ph``, and action, ``a_ph``, and returns the main 
            outputs from the agent&#39;s Tensorflow computation graph:

            ===========  ================  ======================================
            Symbol       Shape             Description
            ===========  ================  ======================================
            ``mu``       (batch, act_dim)  | Computes mean actions from policy
                                           | given states.
            ``pi``       (batch, act_dim)  | Samples actions from policy given 
                                           | states.
            ``logp_pi``  (batch,)          | Gives log probability, according to
                                           | the policy, of the action sampled by
                                           | ``pi``. Critical: must be differentiable
                                           | with respect to policy parameters all
                                           | the way through action sampling.
            ``q1``       (batch,)          | Gives one estimate of Q* for 
                                           | states in ``x_ph`` and actions in
                                           | ``a_ph``.
            ``q2``       (batch,)          | Gives another estimate of Q* for 
                                           | states in ``x_ph`` and actions in
                                           | ``a_ph``.
            ``q1_pi``    (batch,)          | Gives the composition of ``q1`` and 
                                           | ``pi`` for states in ``x_ph``: 
                                           | q1(x, pi(x)).
            ``q2_pi``    (batch,)          | Gives the composition of ``q2`` and 
                                           | ``pi`` for states in ``x_ph``: 
                                           | q2(x, pi(x)).
            ``v``        (batch,)          | Gives the value estimate for states
                                           | in ``x_ph``. 
            ===========  ================  ======================================

        ac_kwargs (dict): Any kwargs appropriate for the actor_critic 
            function you provided to SAC.

        seed (int): Seed for random number generators.

        steps_per_epoch (int): Number of steps of interaction (state-action pairs) 
            for the agent and the environment in each epoch.

        epochs (int): Number of epochs to run and train agent.

        replay_size (int): Maximum length of replay buffer.

        gamma (float): Discount factor. (Always between 0 and 1.)

        polyak (float): Interpolation factor in polyak averaging for target 
            networks. Target networks are updated towards main networks 
            according to:

            .. math:: \\theta_{\\text{targ}} \\leftarrow 
                \\rho \\theta_{\\text{targ}} + (1-\\rho) \\theta

            where :math:`\\rho` is polyak. (Always between 0 and 1, usually 
            close to 1.)

        lr (float): Learning rate (used for both policy and value learning).

        alpha (float): Entropy regularization coefficient. (Equivalent to 
            inverse of reward scale in the original SAC paper.)

        batch_size (int): Minibatch size for SGD.

        start_steps (int): Number of steps for uniform-random action selection,
            before running real policy. Helps exploration.

        max_ep_len (int): Maximum length of trajectory / episode / rollout.

        logger_kwargs (dict): Keyword args for EpochLogger.

        save_freq (int): How often (in terms of gap between epochs) to save
            the current policy and value function.

    &#34;&#34;&#34;


    logger = EpochLogger(**logger_kwargs)
    hyperparams = locals().copy()

    if Teacher:
        del hyperparams[&#39;Teacher&#39;]  # remove teacher to avoid serialization error
        # del hyperparams[&#39;replay_buffer&#39;]
    logger.save_config(hyperparams)

    tf.set_random_seed(seed)
    np.random.seed(seed)

    env, test_env = env_fn(), env_fn()

    if Teacher:
        params = Teacher.set_env_params(env)
    env.reset()

    if not hasattr(env, &#34;env&#34;):
        obs_space = env.observation_space
        act_space = env.action_space
    else:
        obs_space = env.env.observation_space
        act_space = env.env.action_space

    obs_dim = obs_space.shape[0]
    act_dim = act_space.shape[0]

    # Action limit for clamping: critically, assumes all dimensions share the same bound!
    act_limit = act_space.high[0]

    # Share information about action space with policy architecture
    ac_kwargs[&#39;action_space&#39;] = act_space

    # Inputs to computation graph
    x_ph, a_ph, x2_ph, r_ph, d_ph = core.placeholders(obs_dim, act_dim, obs_dim, None, None)

    # Main outputs from computation graph
    with tf.variable_scope(&#39;main&#39;):
        mu, pi, logp_pi, q1, q2, q1_pi, q2_pi, v = actor_critic(x_ph, a_ph, **ac_kwargs)
    
    # Target value network
    with tf.variable_scope(&#39;target&#39;):
        _, _, _, _, _, _, _, v_targ  = actor_critic(x2_ph, a_ph, **ac_kwargs)

    # start Experience buffer (only usefull for episodic replay buffer)
    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)
    # replay_buffer.start_task(params)

    # Count variables
    var_counts = tuple(core.count_vars(scope) for scope in 
                       [&#39;main/pi&#39;, &#39;main/q1&#39;, &#39;main/q2&#39;, &#39;main/v&#39;, &#39;main&#39;])
    print((&#39;\nNumber of parameters: \t pi: %d, \t&#39; + \
           &#39;q1: %d, \t q2: %d, \t v: %d, \t total: %d\n&#39;)%var_counts)

    # Min Double-Q:
    min_q_pi = tf.minimum(q1_pi, q2_pi)

    # Targets for Q and V regression
    q_backup = tf.stop_gradient(r_ph + gamma*(1-d_ph)*v_targ)
    v_backup = tf.stop_gradient(min_q_pi - alpha * logp_pi)

    # Soft actor-critic losses
    pi_loss = tf.reduce_mean(alpha * logp_pi - q1_pi)
    q1_loss = 0.5 * tf.reduce_mean((q_backup - q1)**2)
    q2_loss = 0.5 * tf.reduce_mean((q_backup - q2)**2)
    v_loss = 0.5 * tf.reduce_mean((v_backup - v)**2)
    value_loss = q1_loss + q2_loss + v_loss

    # Policy train op 
    # (has to be separate from value train op, because q1_pi appears in pi_loss)
    pi_optimizer = tf.train.AdamOptimizer(learning_rate=lr)
    train_pi_op = pi_optimizer.minimize(pi_loss, var_list=get_vars(&#39;main/pi&#39;))

    # Value train op
    # (control dep of train_pi_op because sess.run otherwise evaluates in nondeterministic order)
    value_optimizer = tf.train.AdamOptimizer(learning_rate=lr)
    value_params = get_vars(&#39;main/q&#39;) + get_vars(&#39;main/v&#39;)
    with tf.control_dependencies([train_pi_op]):
        train_value_op = value_optimizer.minimize(value_loss, var_list=value_params)

    # Polyak averaging for target variables
    # (control flow because sess.run otherwise evaluates in nondeterministic order)
    with tf.control_dependencies([train_value_op]):
        target_update = tf.group([tf.assign(v_targ, polyak*v_targ + (1-polyak)*v_main)
                                  for v_main, v_targ in zip(get_vars(&#39;main&#39;), get_vars(&#39;target&#39;))])

    # All ops to call during one training step
    step_ops = [pi_loss, q1_loss, q2_loss, v_loss, q1, q2, v, logp_pi, 
                train_pi_op, train_value_op, target_update]

    # Initializing targets to match main variables
    target_init = tf.group([tf.assign(v_targ, v_main)
                              for v_main, v_targ in zip(get_vars(&#39;main&#39;), get_vars(&#39;target&#39;))])

    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True

    if pretrained_model is not None: # set checkpoint weights
        saver = tf.train.Saver()

    sess = tf.Session(config=config)

    if pretrained_model is not None:  # set checkpoint weights
        print(&#34;restoring trained weights&#34;)
        saver.restore(sess, pretrained_model)
        print(&#39;restored&#39;)
    else:
        sess.run(tf.global_variables_initializer())
        sess.run(target_init)

    # Setup model saving
    logger.setup_tf_saver(sess, inputs={&#39;x&#39;: x_ph, &#39;a&#39;: a_ph}, 
                                outputs={&#39;mu&#39;: mu, &#39;pi&#39;: pi, &#39;q1&#39;: q1, &#39;q2&#39;: q2, &#39;v&#39;: v})

    value_estimator_fn = lambda states: np.squeeze(sess.run(v, feed_dict={x_ph: states}))
    Teacher.set_value_estimator(value_estimator_fn)

    def get_action(o, deterministic=False):
        act_op = mu if deterministic else pi
        return sess.run(act_op, feed_dict={x_ph: o.reshape(1,-1)})[0]

    def test_agent(n=10):
        global sess, mu, pi, q1, q2, q1_pi, q2_pi
        for j in range(n):
            if Teacher:
                test_params = Teacher.set_test_env_params(test_env)
            else:
                test_params = None

            try:
                o, r, d, ep_ret, ep_len = test_env.reset(), 0, False, 0, 0
                while not(d or (ep_len == max_ep_len)):
                    # Take deterministic actions at test time
                    o, r, d, _ = test_env.step(get_action(o, True))
                    ep_ret += r
                    ep_len += 1
                logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)
                if Teacher: Teacher.record_test_episode(ep_ret, ep_len)
            except:
                print(&#34;### TEST ERROR ###&#34;)
                print(&#34;Env params: {}&#34;.format(test_params))
                print(&#34;Test nb: {}&#34;.format(j))
                if Teacher:
                    Teacher.dump(logger.output_dir + &#39;env_params_save.pkl&#39;)
                logger.save_state({&#39;env&#39;: env}, None)
                raise

    start_time = time.time()
    o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0
    Teacher.record_train_task_initial_state(o)
    total_steps = steps_per_epoch * epochs

    # Main loop: collect experience in env and update/log each epoch
    for t in range(total_steps):
        &#34;&#34;&#34;
        Until start_steps have elapsed, randomly sample actions
        from a uniform distribution for better exploration. Afterwards, 
        use the learned policy. 
        &#34;&#34;&#34;
        if t &gt; start_steps:
            a = get_action(o)
        else:
            a = act_space.sample()

        # Step the env

        o2, r, d, infos = env.step(a)
        ep_ret += r
        ep_len += 1
        Teacher.record_train_step(o, a, r, o2, d)

        # Ignore the &#34;done&#34; signal if it comes from hitting the time
        # horizon (that is, when it&#39;s an artificial terminal signal
        # that isn&#39;t based on the agent&#39;s state)
        d = False if ep_len==max_ep_len else d

        # Store experience to replay buffer
        replay_buffer.store(o, a, r, o2, d)

        # Super critical, easy to overlook step: make sure to update 
        # most recent observation!
        o = o2

        if d or (ep_len == max_ep_len):
            # replay_buffer.end_task(ep_ret, ep_len)
            &#34;&#34;&#34;
            Perform all SAC updates at the end of the trajectory.
            This is a slight difference from the SAC specified in the
            original paper.
            &#34;&#34;&#34;
            for j in range(np.ceil(ep_len/train_freq).astype(&#39;int&#39;)):
                batch = replay_buffer.sample_batch(batch_size)
                feed_dict = {x_ph: batch[&#39;obs1&#39;],
                             x2_ph: batch[&#39;obs2&#39;],
                             a_ph: batch[&#39;acts&#39;],
                             r_ph: batch[&#39;rews&#39;],
                             d_ph: batch[&#39;done&#39;],
                            }
                outs = sess.run(step_ops, feed_dict)
                # logger.store(LossPi=outs[0], LossQ1=outs[1], LossQ2=outs[2],
                #              LossV=outs[3], Q1Vals=outs[4], Q2Vals=outs[5],
                #              VVals=outs[6], LogPi=outs[7])
            logger.store(EpRet=ep_ret, EpLen=ep_len)
            if Teacher:
                success = False if &#39;success&#39; not in infos else infos[&#34;success&#34;]
                Teacher.record_train_episode(ep_ret, ep_len, success)
                params = Teacher.set_env_params(env)
                # replay_buffer.start_task(params)
            o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0
            Teacher.record_train_task_initial_state(o)

        # End of epoch wrap-up
        if t &gt; 0 and (t + 1) % steps_per_epoch == 0:
            epoch = (t + 1) // steps_per_epoch

            # Save model
            if epoch % save_freq == 0:  # or (epoch == epochs):
                if half_save and epoch == epochs/2:
                    logger.save_state({&#39;env&#39;: env, &#39;test_env&#39;: test_env}, None)#itr=epoch)
                else:
                    logger.save_state({&#39;env&#39;: env, &#39;test_env&#39;: test_env}, None)#itr=epoch)

            # Test the performance of the deterministic version of the agent.
            test_agent(n=nb_test_episodes)
            # Log info about epoch
            logger.log_tabular(&#39;Epoch&#39;, epoch)
            logger.log_tabular(&#39;EpRet&#39;, with_min_and_max=True)
            logger.log_tabular(&#39;TestEpRet&#39;, with_min_and_max=True)
            logger.log_tabular(&#39;EpLen&#39;, average_only=True)
            logger.log_tabular(&#39;TestEpLen&#39;, average_only=True)
            logger.log_tabular(&#39;TotalEnvInteracts&#39;, t+1)
            #logger.log_tabular(&#39;Q1Vals&#39;, with_min_and_max=True)
            #logger.log_tabular(&#39;Q2Vals&#39;, with_min_and_max=True)
            #logger.log_tabular(&#39;VVals&#39;, with_min_and_max=True)
            #logger.log_tabular(&#39;LogPi&#39;, with_min_and_max=True)
            #logger.log_tabular(&#39;LossPi&#39;, average_only=True)
            #logger.log_tabular(&#39;LossQ1&#39;, average_only=True)
            #logger.log_tabular(&#39;LossQ2&#39;, average_only=True)
            #logger.log_tabular(&#39;LossV&#39;, average_only=True)
            logger.log_tabular(&#39;Time&#39;, time.time()-start_time)
            logger.dump_tabular()

            # Pickle parameterized env data
            #print(logger.output_dir+&#39;/env_params_save.pkl&#39;)
            if Teacher: Teacher.dump(logger.output_dir+&#39;/env_params_save.pkl&#39;)

        #### RESET ####
        if reset_frequency is not None and t &gt; 0 and t % reset_frequency == 0:
            print(&#34;Reset student.&#34;)
            sess.run(tf.global_variables_initializer())
            sess.run(target_init)
            replay_buffer.ptr, replay_buffer.size = 0, 0  # reset replay buffer</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="TeachMyAgent.students.spinup.algos.tf1.sac_v011.sac.ReplayBuffer"><code class="flex name class">
<span>class <span class="ident">ReplayBuffer</span></span>
<span>(</span><span>obs_dim, act_dim, size)</span>
</code></dt>
<dd>
<div class="desc"><p>A simple FIFO experience replay buffer for SAC agents.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ReplayBuffer:
    &#34;&#34;&#34;
    A simple FIFO experience replay buffer for SAC agents.
    &#34;&#34;&#34;

    def __init__(self, obs_dim, act_dim, size):
        self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)
        self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)
        self.acts_buf = np.zeros([size, act_dim], dtype=np.float32)
        self.rews_buf = np.zeros(size, dtype=np.float32)
        self.done_buf = np.zeros(size, dtype=np.float32)
        self.ptr, self.size, self.max_size = 0, 0, size

    def store(self, obs, act, rew, next_obs, done):
        self.obs1_buf[self.ptr] = obs
        self.obs2_buf[self.ptr] = next_obs
        self.acts_buf[self.ptr] = act
        self.rews_buf[self.ptr] = rew
        self.done_buf[self.ptr] = done
        self.ptr = (self.ptr+1) % self.max_size
        self.size = min(self.size+1, self.max_size)

    def sample_batch(self, batch_size=32):
        idxs = np.random.randint(0, self.size, size=batch_size)
        return dict(obs1=self.obs1_buf[idxs],
                    obs2=self.obs2_buf[idxs],
                    acts=self.acts_buf[idxs],
                    rews=self.rews_buf[idxs],
                    done=self.done_buf[idxs])</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.spinup.algos.tf1.sac_v011.sac.ReplayBuffer.sample_batch"><code class="name flex">
<span>def <span class="ident">sample_batch</span></span>(<span>self, batch_size=32)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_batch(self, batch_size=32):
    idxs = np.random.randint(0, self.size, size=batch_size)
    return dict(obs1=self.obs1_buf[idxs],
                obs2=self.obs2_buf[idxs],
                acts=self.acts_buf[idxs],
                rews=self.rews_buf[idxs],
                done=self.done_buf[idxs])</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.spinup.algos.tf1.sac_v011.sac.ReplayBuffer.store"><code class="name flex">
<span>def <span class="ident">store</span></span>(<span>self, obs, act, rew, next_obs, done)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def store(self, obs, act, rew, next_obs, done):
    self.obs1_buf[self.ptr] = obs
    self.obs2_buf[self.ptr] = next_obs
    self.acts_buf[self.ptr] = act
    self.rews_buf[self.ptr] = rew
    self.done_buf[self.ptr] = done
    self.ptr = (self.ptr+1) % self.max_size
    self.size = min(self.size+1, self.max_size)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<a href="http://developmentalsystems.org/TeachMyAgent/doc/">
<img src="https://github.com/flowersteam/TeachMyAgent/blob/gh-pages/images/home/head_image.png?raw=true" style="display: block; margin: 1em auto">
</a>
<a href="http://developmentalsystems.org/TeachMyAgent/doc/">Home</a> | <a href="http://developmentalsystems.org/TeachMyAgent/">Website</a>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="TeachMyAgent.students.spinup.algos.tf1.sac_v011" href="index.html">TeachMyAgent.students.spinup.algos.tf1.sac_v011</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="TeachMyAgent.students.spinup.algos.tf1.sac_v011.sac.sac" href="#TeachMyAgent.students.spinup.algos.tf1.sac_v011.sac.sac">sac</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="TeachMyAgent.students.spinup.algos.tf1.sac_v011.sac.ReplayBuffer" href="#TeachMyAgent.students.spinup.algos.tf1.sac_v011.sac.ReplayBuffer">ReplayBuffer</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.students.spinup.algos.tf1.sac_v011.sac.ReplayBuffer.sample_batch" href="#TeachMyAgent.students.spinup.algos.tf1.sac_v011.sac.ReplayBuffer.sample_batch">sample_batch</a></code></li>
<li><code><a title="TeachMyAgent.students.spinup.algos.tf1.sac_v011.sac.ReplayBuffer.store" href="#TeachMyAgent.students.spinup.algos.tf1.sac_v011.sac.ReplayBuffer.store">store</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>