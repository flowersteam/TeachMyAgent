<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>TeachMyAgent.students.spinup.algos.pytorch.sac_v02.sac API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>TeachMyAgent.students.spinup.algos.pytorch.sac_v02.sac</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from copy import deepcopy
import itertools
import numpy as np
import torch
from torch.optim import Adam
import gym
import time
from TeachMyAgent.students.spinup.algos.pytorch.sac_v02 import core
from TeachMyAgent.students.spinup.utils.logx import EpochLogger

class ReplayBuffer:
    &#34;&#34;&#34;
    A simple FIFO experience replay buffer for SAC agents.
    &#34;&#34;&#34;

    def __init__(self, obs_dim, act_dim, size):
        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)
        self.obs2_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)
        self.act_buf = np.zeros(core.combined_shape(size, act_dim), dtype=np.float32)
        self.rew_buf = np.zeros(size, dtype=np.float32)
        self.done_buf = np.zeros(size, dtype=np.float32)
        self.ptr, self.size, self.max_size = 0, 0, size

    def store(self, obs, act, rew, next_obs, done):
        self.obs_buf[self.ptr] = obs
        self.obs2_buf[self.ptr] = next_obs
        self.act_buf[self.ptr] = act
        self.rew_buf[self.ptr] = rew
        self.done_buf[self.ptr] = done
        self.ptr = (self.ptr+1) % self.max_size
        self.size = min(self.size+1, self.max_size)

    def sample_batch(self, batch_size=32):
        idxs = np.random.randint(0, self.size, size=batch_size)
        batch = dict(obs=self.obs_buf[idxs],
                     obs2=self.obs2_buf[idxs],
                     act=self.act_buf[idxs],
                     rew=self.rew_buf[idxs],
                     done=self.done_buf[idxs])
        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}


def sac(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0,
        steps_per_epoch=4000, epochs=100, replay_size=int(1e6), gamma=0.99,
        polyak=0.995, lr=1e-3, alpha=0.2, batch_size=100, start_steps=10000,
        update_after=1000, update_every=50, num_test_episodes=10, max_ep_len=1000,
        logger_kwargs=dict(), save_freq=1, Teacher=None, half_save=False, pretrained_model=None, reset_frequency=None):
    &#34;&#34;&#34;
    Soft Actor-Critic (SAC)

    With some modifications were made to make it use an ACL teacher and a test env.

    Args:
        env_fn : A function which creates a copy of the environment.
            The environment must satisfy the OpenAI Gym API.

        actor_critic: The constructor method for a PyTorch Module with an ``act`` 
            method, a ``pi`` module, a ``q1`` module, and a ``q2`` module.
            The ``act`` method and ``pi`` module should accept batches of 
            observations as inputs, and ``q1`` and ``q2`` should accept a batch 
            of observations and a batch of actions as inputs. When called, 
            ``act``, ``q1``, and ``q2`` should return:

            ===========  ================  ======================================
            Call         Output Shape      Description
            ===========  ================  ======================================
            ``act``      (batch, act_dim)  | Numpy array of actions for each 
                                           | observation.
            ``q1``       (batch,)          | Tensor containing one current estimate
                                           | of Q* for the provided observations
                                           | and actions. (Critical: make sure to
                                           | flatten this!)
            ``q2``       (batch,)          | Tensor containing the other current 
                                           | estimate of Q* for the provided observations
                                           | and actions. (Critical: make sure to
                                           | flatten this!)
            ===========  ================  ======================================

            Calling ``pi`` should return:

            ===========  ================  ======================================
            Symbol       Shape             Description
            ===========  ================  ======================================
            ``a``        (batch, act_dim)  | Tensor containing actions from policy
                                           | given observations.
            ``logp_pi``  (batch,)          | Tensor containing log probabilities of
                                           | actions in ``a``. Importantly: gradients
                                           | should be able to flow back into ``a``.
            ===========  ================  ======================================

        ac_kwargs (dict): Any kwargs appropriate for the ActorCritic object 
            you provided to SAC.

        seed (int): Seed for random number generators.

        steps_per_epoch (int): Number of steps of interaction (state-action pairs) 
            for the agent and the environment in each epoch.

        epochs (int): Number of epochs to run and train agent.

        replay_size (int): Maximum length of replay buffer.

        gamma (float): Discount factor. (Always between 0 and 1.)

        polyak (float): Interpolation factor in polyak averaging for target 
            networks. Target networks are updated towards main networks 
            according to:

            .. math:: \\theta_{\\text{targ}} \\leftarrow 
                \\rho \\theta_{\\text{targ}} + (1-\\rho) \\theta

            where :math:`\\rho` is polyak. (Always between 0 and 1, usually 
            close to 1.)

        lr (float): Learning rate (used for both policy and value learning).

        alpha (float): Entropy regularization coefficient. (Equivalent to 
            inverse of reward scale in the original SAC paper.)

        batch_size (int): Minibatch size for SGD.

        start_steps (int): Number of steps for uniform-random action selection,
            before running real policy. Helps exploration.

        update_after (int): Number of env interactions to collect before
            starting to do gradient descent updates. Ensures replay buffer
            is full enough for useful updates.

        update_every (int): Number of env interactions that should elapse
            between gradient descent updates. Note: Regardless of how long 
            you wait between updates, the ratio of env steps to gradient steps 
            is locked to 1.

        num_test_episodes (int): Number of episodes to test the deterministic
            policy at the end of each epoch.

        max_ep_len (int): Maximum length of trajectory / episode / rollout.

        logger_kwargs (dict): Keyword args for EpochLogger.

        save_freq (int): How often (in terms of gap between epochs) to save
            the current policy and value function.

    &#34;&#34;&#34;
    if reset_frequency is not None:
        raise Exception(&#34;Resetting student functionnality is currently not implemented on this student.&#34;)
    logger = EpochLogger(**logger_kwargs)
    hyperparams = locals().copy()
    if Teacher:
        del hyperparams[&#39;Teacher&#39;]  # remove teacher to avoid serialization error
    logger.save_config(hyperparams)

    torch.manual_seed(seed)
    np.random.seed(seed)

    env, test_env = env_fn(), env_fn()

    if Teacher:
        params = Teacher.set_env_params(env)
    env.reset()

    obs_dim = env.observation_space.shape
    act_dim = env.action_space.shape[0]

    # Action limit for clamping: critically, assumes all dimensions share the same bound!
    act_limit = env.action_space.high[0]

    # Create actor-critic module and target networks
    ac = actor_critic(env.observation_space, env.action_space, **ac_kwargs)
    ac_targ = deepcopy(ac)

    value_estimator_fn = lambda states: np.sum(ac.q1(states, np.array([0]*act_dim)))  # No value estimator !!!
    Teacher.set_value_estimator(value_estimator_fn)

    # Freeze target networks with respect to optimizers (only update via polyak averaging)
    for p in ac_targ.parameters():
        p.requires_grad = False
        
    # List of parameters for both Q-networks (save this for convenience)
    q_params = itertools.chain(ac.q1.parameters(), ac.q2.parameters())

    # start Experience buffer (only usefull for episodic replay buffer)
    #replay_buffer.start_task(params)
    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)

    # Count variables (protip: try to get a feel for how different size networks behave!)
    var_counts = tuple(core.count_vars(module) for module in [ac.pi, ac.q1, ac.q2])
    logger.log(&#39;\nNumber of parameters: \t pi: %d, \t q1: %d, \t q2: %d\n&#39;%var_counts)

    # Set up function for computing SAC Q-losses
    def compute_loss_q(data):
        o, a, r, o2, d = data[&#39;obs&#39;], data[&#39;act&#39;], data[&#39;rew&#39;], data[&#39;obs2&#39;], data[&#39;done&#39;]

        q1 = ac.q1(o,a)
        q2 = ac.q2(o,a)

        # Bellman backup for Q functions
        with torch.no_grad():
            # Target actions come from *current* policy
            a2, logp_a2 = ac.pi(o2)

            # Target Q-values
            q1_pi_targ = ac_targ.q1(o2, a2)
            q2_pi_targ = ac_targ.q2(o2, a2)
            q_pi_targ = torch.min(q1_pi_targ, q2_pi_targ)
            backup = r + gamma * (1 - d) * (q_pi_targ - alpha * logp_a2)

        # MSE loss against Bellman backup
        loss_q1 = ((q1 - backup)**2).mean()
        loss_q2 = ((q2 - backup)**2).mean()
        loss_q = loss_q1 + loss_q2

        # Useful info for logging
        q_info = dict(Q1Vals=q1.detach().numpy(),
                      Q2Vals=q2.detach().numpy())

        return loss_q, q_info

    # Set up function for computing SAC pi loss
    def compute_loss_pi(data):
        o = data[&#39;obs&#39;]
        pi, logp_pi = ac.pi(o)
        q1_pi = ac.q1(o, pi)
        q2_pi = ac.q2(o, pi)
        q_pi = torch.min(q1_pi, q2_pi)

        # Entropy-regularized policy loss
        loss_pi = (alpha * logp_pi - q_pi).mean()

        # Useful info for logging
        pi_info = dict(LogPi=logp_pi.detach().numpy())

        return loss_pi, pi_info

    # Set up optimizers for policy and q-function
    pi_optimizer = Adam(ac.pi.parameters(), lr=lr)
    q_optimizer = Adam(q_params, lr=lr)

    # Handle pretrained model
    # TODO : To test !
    if pretrained_model is not None:  # set checkpoint weights
        print(&#34;restoring trained weights&#34;)
        checkpoint = torch.load(pretrained_model)
        ac.load_state_dict(checkpoint[&#39;ac&#39;])
        ac_targ.load_state_dict(checkpoint[&#39;ac_targ&#39;])
        pi_optimizer.load_state_dict(checkpoint[&#39;pi_optimizer&#39;])
        q_optimizer.load_state_dict(checkpoint[&#39;q_optimizer&#39;])

        print(&#39;restored&#39;)

    # Set up model saving
    #logger.setup_pytorch_saver(ac)
    logger.setup_pytorch_saver({
        &#34;ac&#34;: ac.state_dict(),
        &#34;ac_targ&#34;: ac_targ.state_dict(),
        &#34;pi_optimizer&#34;: pi_optimizer.state_dict(),
        &#34;q_optimizer&#34;: q_optimizer.state_dict()
    })

    def update(data):
        # First run one gradient descent step for Q1 and Q2
        q_optimizer.zero_grad()
        loss_q, q_info = compute_loss_q(data)
        loss_q.backward()
        q_optimizer.step()

        # Record things
        logger.store(LossQ=loss_q.item(), **q_info)

        # Freeze Q-networks so you don&#39;t waste computational effort 
        # computing gradients for them during the policy learning step.
        for p in q_params:
            p.requires_grad = False

        # Next run one gradient descent step for pi.
        pi_optimizer.zero_grad()
        loss_pi, pi_info = compute_loss_pi(data)
        loss_pi.backward()
        pi_optimizer.step()

        # Unfreeze Q-networks so you can optimize it at next DDPG step.
        for p in q_params:
            p.requires_grad = True

        # Record things
        logger.store(LossPi=loss_pi.item(), **pi_info)

        # Finally, update target networks by polyak averaging.
        with torch.no_grad():
            for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):
                # NB: We use an in-place operations &#34;mul_&#34;, &#34;add_&#34; to update target
                # params, as opposed to &#34;mul&#34; and &#34;add&#34;, which would make new tensors.
                p_targ.data.mul_(polyak)
                p_targ.data.add_((1 - polyak) * p.data)

    def get_action(o, deterministic=False):
        return ac.act(torch.as_tensor(o, dtype=torch.float32), 
                      deterministic)

    def test_agent():
        for j in range(num_test_episodes):
            if Teacher: Teacher.set_test_env_params(test_env)
            o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0
            while not(d or (ep_len == max_ep_len)):
                # Take deterministic actions at test time 
                o, r, d, _ = test_env.step(get_action(o, True))
                ep_ret += r
                ep_len += 1
            logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)
            if Teacher: Teacher.record_test_episode(ep_ret, ep_len)

    # Prepare for interaction with environment
    total_steps = steps_per_epoch * epochs
    start_time = time.time()
    o, ep_ret, ep_len = env.reset(), 0, 0
    Teacher.record_train_task_initial_state(o)

    # Main loop: collect experience in env and update/log each epoch
    for t in range(total_steps):
        
        # Until start_steps have elapsed, randomly sample actions
        # from a uniform distribution for better exploration. Afterwards, 
        # use the learned policy. 
        if t &gt; start_steps:
            a = get_action(o)
        else:
            a = env.action_space.sample()

        # Step the env
        o2, r, d, infos = env.step(a)
        ep_ret += r
        ep_len += 1
        Teacher.record_train_step(o, a, r, o2, d)

        # Ignore the &#34;done&#34; signal if it comes from hitting the time
        # horizon (that is, when it&#39;s an artificial terminal signal
        # that isn&#39;t based on the agent&#39;s state)
        d = False if ep_len==max_ep_len else d

        # Store experience to replay buffer
        replay_buffer.store(o, a, r, o2, d)

        # Super critical, easy to overlook step: make sure to update 
        # most recent observation!
        o = o2

        # End of trajectory handling
        if d or (ep_len == max_ep_len):
            logger.store(EpRet=ep_ret, EpLen=ep_len)
            if Teacher:
                success = False if &#39;success&#39; not in infos else infos[&#34;success&#34;]
                Teacher.record_train_episode(ep_ret, ep_len, success)
                params = Teacher.set_env_params(env)
            o, ep_ret, ep_len = env.reset(), 0, 0
            Teacher.record_train_task_initial_state(o)

        # Update handling
        if t &gt;= update_after and t % update_every == 0:
            #for j in range(update_every):
            batch = replay_buffer.sample_batch(batch_size)
            update(data=batch)

        # End of epoch handling
        if (t+1) % steps_per_epoch == 0:
            epoch = (t+1) // steps_per_epoch

            # Save model
            if epoch % save_freq == 0:  # or (epoch == epochs):
                if half_save and epoch == epochs / 2:
                    logger.save_state({&#39;env&#39;: env}, itr=epoch)
                else:
                    logger.save_state({&#39;env&#39;: env}, None)  # itr=epoch)

            # Test the performance of the deterministic version of the agent.
            test_agent()

            # Log info about epoch
            logger.log_tabular(&#39;Epoch&#39;, epoch)
            logger.log_tabular(&#39;EpRet&#39;, with_min_and_max=True)
            logger.log_tabular(&#39;TestEpRet&#39;, with_min_and_max=True)
            logger.log_tabular(&#39;EpLen&#39;, average_only=True)
            logger.log_tabular(&#39;TestEpLen&#39;, average_only=True)
            logger.log_tabular(&#39;TotalEnvInteracts&#39;, t)
            logger.log_tabular(&#39;Q1Vals&#39;, with_min_and_max=True)
            logger.log_tabular(&#39;Q2Vals&#39;, with_min_and_max=True)
            logger.log_tabular(&#39;LogPi&#39;, with_min_and_max=True)
            logger.log_tabular(&#39;LossPi&#39;, average_only=True)
            logger.log_tabular(&#39;LossQ&#39;, average_only=True)
            logger.log_tabular(&#39;Time&#39;, time.time()-start_time)
            logger.dump_tabular()

            # Pickle parameterized env data
            # print(logger.output_dir+&#39;/env_params_save.pkl&#39;)
            if Teacher: Teacher.dump(logger.output_dir + &#39;/env_params_save.pkl&#39;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="TeachMyAgent.students.spinup.algos.pytorch.sac_v02.sac.sac"><code class="name flex">
<span>def <span class="ident">sac</span></span>(<span>env_fn, actor_critic=TeachMyAgent.students.spinup.algos.pytorch.sac_v02.core.MLPActorCritic, ac_kwargs={}, seed=0, steps_per_epoch=4000, epochs=100, replay_size=1000000, gamma=0.99, polyak=0.995, lr=0.001, alpha=0.2, batch_size=100, start_steps=10000, update_after=1000, update_every=50, num_test_episodes=10, max_ep_len=1000, logger_kwargs={}, save_freq=1, Teacher=None, half_save=False, pretrained_model=None, reset_frequency=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Soft Actor-Critic (SAC)</p>
<p>With some modifications were made to make it use an ACL teacher and a test env.</p>
<h2 id="args">Args</h2>
<p>env_fn : A function which creates a copy of the environment.
The environment must satisfy the OpenAI Gym API.</p>
<dl>
<dt><strong><code>actor_critic</code></strong></dt>
<dd>
<p>The constructor method for a PyTorch Module with an <code>act</code>
method, a <code>pi</code> module, a <code>q1</code> module, and a <code>q2</code> module.
The <code>act</code> method and <code>pi</code> module should accept batches of
observations as inputs, and <code>q1</code> and <code>q2</code> should accept a batch
of observations and a batch of actions as inputs. When called,
<code>act</code>, <code>q1</code>, and <code>q2</code> should return:</p>
<p>===========
================
======================================
Call
Output Shape
Description
===========
================
======================================
<code>act</code>
(batch, act_dim)
| Numpy array of actions for each
| observation.
<code>q1</code>
(batch,)
| Tensor containing one current estimate
| of Q<em> for the provided observations
| and actions. (Critical: make sure to
| flatten this!)
<code>q2</code>
(batch,)
| Tensor containing the other current
| estimate of Q</em> for the provided observations
| and actions. (Critical: make sure to
| flatten this!)
===========
================
======================================</p>
<p>Calling <code>pi</code> should return:</p>
<p>===========
================
======================================
Symbol
Shape
Description
===========
================
======================================
<code>a</code>
(batch, act_dim)
| Tensor containing actions from policy
| given observations.
<code>logp_pi</code>
(batch,)
| Tensor containing log probabilities of
| actions in <code>a</code>. Importantly: gradients
| should be able to flow back into <code>a</code>.
===========
================
======================================</p>
</dd>
<dt><strong><code>ac_kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Any kwargs appropriate for the ActorCritic object
you provided to SAC.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Seed for random number generators.</dd>
<dt><strong><code>steps_per_epoch</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of steps of interaction (state-action pairs)
for the agent and the environment in each epoch.</dd>
<dt><strong><code>epochs</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of epochs to run and train agent.</dd>
<dt><strong><code>replay_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum length of replay buffer.</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code></dt>
<dd>Discount factor. (Always between 0 and 1.)</dd>
<dt><strong><code>polyak</code></strong> :&ensp;<code>float</code></dt>
<dd>
<p>Interpolation factor in polyak averaging for target
networks. Target networks are updated towards main networks
according to:</p>
<p>[ \rho \theta_{\text{targ}} + (1-\rho) \theta ]
where :math:<code>\rho</code> is polyak. (Always between 0 and 1, usually
close to 1.)</p>
</dd>
<dt><strong><code>lr</code></strong> :&ensp;<code>float</code></dt>
<dd>Learning rate (used for both policy and value learning).</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>Entropy regularization coefficient. (Equivalent to
inverse of reward scale in the original SAC paper.)</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Minibatch size for SGD.</dd>
<dt><strong><code>start_steps</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of steps for uniform-random action selection,
before running real policy. Helps exploration.</dd>
<dt><strong><code>update_after</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of env interactions to collect before
starting to do gradient descent updates. Ensures replay buffer
is full enough for useful updates.</dd>
<dt><strong><code>update_every</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of env interactions that should elapse
between gradient descent updates. Note: Regardless of how long
you wait between updates, the ratio of env steps to gradient steps
is locked to 1.</dd>
<dt><strong><code>num_test_episodes</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of episodes to test the deterministic
policy at the end of each epoch.</dd>
<dt><strong><code>max_ep_len</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum length of trajectory / episode / rollout.</dd>
<dt><strong><code>logger_kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Keyword args for EpochLogger.</dd>
<dt><strong><code>save_freq</code></strong> :&ensp;<code>int</code></dt>
<dd>How often (in terms of gap between epochs) to save
the current policy and value function.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sac(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0,
        steps_per_epoch=4000, epochs=100, replay_size=int(1e6), gamma=0.99,
        polyak=0.995, lr=1e-3, alpha=0.2, batch_size=100, start_steps=10000,
        update_after=1000, update_every=50, num_test_episodes=10, max_ep_len=1000,
        logger_kwargs=dict(), save_freq=1, Teacher=None, half_save=False, pretrained_model=None, reset_frequency=None):
    &#34;&#34;&#34;
    Soft Actor-Critic (SAC)

    With some modifications were made to make it use an ACL teacher and a test env.

    Args:
        env_fn : A function which creates a copy of the environment.
            The environment must satisfy the OpenAI Gym API.

        actor_critic: The constructor method for a PyTorch Module with an ``act`` 
            method, a ``pi`` module, a ``q1`` module, and a ``q2`` module.
            The ``act`` method and ``pi`` module should accept batches of 
            observations as inputs, and ``q1`` and ``q2`` should accept a batch 
            of observations and a batch of actions as inputs. When called, 
            ``act``, ``q1``, and ``q2`` should return:

            ===========  ================  ======================================
            Call         Output Shape      Description
            ===========  ================  ======================================
            ``act``      (batch, act_dim)  | Numpy array of actions for each 
                                           | observation.
            ``q1``       (batch,)          | Tensor containing one current estimate
                                           | of Q* for the provided observations
                                           | and actions. (Critical: make sure to
                                           | flatten this!)
            ``q2``       (batch,)          | Tensor containing the other current 
                                           | estimate of Q* for the provided observations
                                           | and actions. (Critical: make sure to
                                           | flatten this!)
            ===========  ================  ======================================

            Calling ``pi`` should return:

            ===========  ================  ======================================
            Symbol       Shape             Description
            ===========  ================  ======================================
            ``a``        (batch, act_dim)  | Tensor containing actions from policy
                                           | given observations.
            ``logp_pi``  (batch,)          | Tensor containing log probabilities of
                                           | actions in ``a``. Importantly: gradients
                                           | should be able to flow back into ``a``.
            ===========  ================  ======================================

        ac_kwargs (dict): Any kwargs appropriate for the ActorCritic object 
            you provided to SAC.

        seed (int): Seed for random number generators.

        steps_per_epoch (int): Number of steps of interaction (state-action pairs) 
            for the agent and the environment in each epoch.

        epochs (int): Number of epochs to run and train agent.

        replay_size (int): Maximum length of replay buffer.

        gamma (float): Discount factor. (Always between 0 and 1.)

        polyak (float): Interpolation factor in polyak averaging for target 
            networks. Target networks are updated towards main networks 
            according to:

            .. math:: \\theta_{\\text{targ}} \\leftarrow 
                \\rho \\theta_{\\text{targ}} + (1-\\rho) \\theta

            where :math:`\\rho` is polyak. (Always between 0 and 1, usually 
            close to 1.)

        lr (float): Learning rate (used for both policy and value learning).

        alpha (float): Entropy regularization coefficient. (Equivalent to 
            inverse of reward scale in the original SAC paper.)

        batch_size (int): Minibatch size for SGD.

        start_steps (int): Number of steps for uniform-random action selection,
            before running real policy. Helps exploration.

        update_after (int): Number of env interactions to collect before
            starting to do gradient descent updates. Ensures replay buffer
            is full enough for useful updates.

        update_every (int): Number of env interactions that should elapse
            between gradient descent updates. Note: Regardless of how long 
            you wait between updates, the ratio of env steps to gradient steps 
            is locked to 1.

        num_test_episodes (int): Number of episodes to test the deterministic
            policy at the end of each epoch.

        max_ep_len (int): Maximum length of trajectory / episode / rollout.

        logger_kwargs (dict): Keyword args for EpochLogger.

        save_freq (int): How often (in terms of gap between epochs) to save
            the current policy and value function.

    &#34;&#34;&#34;
    if reset_frequency is not None:
        raise Exception(&#34;Resetting student functionnality is currently not implemented on this student.&#34;)
    logger = EpochLogger(**logger_kwargs)
    hyperparams = locals().copy()
    if Teacher:
        del hyperparams[&#39;Teacher&#39;]  # remove teacher to avoid serialization error
    logger.save_config(hyperparams)

    torch.manual_seed(seed)
    np.random.seed(seed)

    env, test_env = env_fn(), env_fn()

    if Teacher:
        params = Teacher.set_env_params(env)
    env.reset()

    obs_dim = env.observation_space.shape
    act_dim = env.action_space.shape[0]

    # Action limit for clamping: critically, assumes all dimensions share the same bound!
    act_limit = env.action_space.high[0]

    # Create actor-critic module and target networks
    ac = actor_critic(env.observation_space, env.action_space, **ac_kwargs)
    ac_targ = deepcopy(ac)

    value_estimator_fn = lambda states: np.sum(ac.q1(states, np.array([0]*act_dim)))  # No value estimator !!!
    Teacher.set_value_estimator(value_estimator_fn)

    # Freeze target networks with respect to optimizers (only update via polyak averaging)
    for p in ac_targ.parameters():
        p.requires_grad = False
        
    # List of parameters for both Q-networks (save this for convenience)
    q_params = itertools.chain(ac.q1.parameters(), ac.q2.parameters())

    # start Experience buffer (only usefull for episodic replay buffer)
    #replay_buffer.start_task(params)
    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)

    # Count variables (protip: try to get a feel for how different size networks behave!)
    var_counts = tuple(core.count_vars(module) for module in [ac.pi, ac.q1, ac.q2])
    logger.log(&#39;\nNumber of parameters: \t pi: %d, \t q1: %d, \t q2: %d\n&#39;%var_counts)

    # Set up function for computing SAC Q-losses
    def compute_loss_q(data):
        o, a, r, o2, d = data[&#39;obs&#39;], data[&#39;act&#39;], data[&#39;rew&#39;], data[&#39;obs2&#39;], data[&#39;done&#39;]

        q1 = ac.q1(o,a)
        q2 = ac.q2(o,a)

        # Bellman backup for Q functions
        with torch.no_grad():
            # Target actions come from *current* policy
            a2, logp_a2 = ac.pi(o2)

            # Target Q-values
            q1_pi_targ = ac_targ.q1(o2, a2)
            q2_pi_targ = ac_targ.q2(o2, a2)
            q_pi_targ = torch.min(q1_pi_targ, q2_pi_targ)
            backup = r + gamma * (1 - d) * (q_pi_targ - alpha * logp_a2)

        # MSE loss against Bellman backup
        loss_q1 = ((q1 - backup)**2).mean()
        loss_q2 = ((q2 - backup)**2).mean()
        loss_q = loss_q1 + loss_q2

        # Useful info for logging
        q_info = dict(Q1Vals=q1.detach().numpy(),
                      Q2Vals=q2.detach().numpy())

        return loss_q, q_info

    # Set up function for computing SAC pi loss
    def compute_loss_pi(data):
        o = data[&#39;obs&#39;]
        pi, logp_pi = ac.pi(o)
        q1_pi = ac.q1(o, pi)
        q2_pi = ac.q2(o, pi)
        q_pi = torch.min(q1_pi, q2_pi)

        # Entropy-regularized policy loss
        loss_pi = (alpha * logp_pi - q_pi).mean()

        # Useful info for logging
        pi_info = dict(LogPi=logp_pi.detach().numpy())

        return loss_pi, pi_info

    # Set up optimizers for policy and q-function
    pi_optimizer = Adam(ac.pi.parameters(), lr=lr)
    q_optimizer = Adam(q_params, lr=lr)

    # Handle pretrained model
    # TODO : To test !
    if pretrained_model is not None:  # set checkpoint weights
        print(&#34;restoring trained weights&#34;)
        checkpoint = torch.load(pretrained_model)
        ac.load_state_dict(checkpoint[&#39;ac&#39;])
        ac_targ.load_state_dict(checkpoint[&#39;ac_targ&#39;])
        pi_optimizer.load_state_dict(checkpoint[&#39;pi_optimizer&#39;])
        q_optimizer.load_state_dict(checkpoint[&#39;q_optimizer&#39;])

        print(&#39;restored&#39;)

    # Set up model saving
    #logger.setup_pytorch_saver(ac)
    logger.setup_pytorch_saver({
        &#34;ac&#34;: ac.state_dict(),
        &#34;ac_targ&#34;: ac_targ.state_dict(),
        &#34;pi_optimizer&#34;: pi_optimizer.state_dict(),
        &#34;q_optimizer&#34;: q_optimizer.state_dict()
    })

    def update(data):
        # First run one gradient descent step for Q1 and Q2
        q_optimizer.zero_grad()
        loss_q, q_info = compute_loss_q(data)
        loss_q.backward()
        q_optimizer.step()

        # Record things
        logger.store(LossQ=loss_q.item(), **q_info)

        # Freeze Q-networks so you don&#39;t waste computational effort 
        # computing gradients for them during the policy learning step.
        for p in q_params:
            p.requires_grad = False

        # Next run one gradient descent step for pi.
        pi_optimizer.zero_grad()
        loss_pi, pi_info = compute_loss_pi(data)
        loss_pi.backward()
        pi_optimizer.step()

        # Unfreeze Q-networks so you can optimize it at next DDPG step.
        for p in q_params:
            p.requires_grad = True

        # Record things
        logger.store(LossPi=loss_pi.item(), **pi_info)

        # Finally, update target networks by polyak averaging.
        with torch.no_grad():
            for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):
                # NB: We use an in-place operations &#34;mul_&#34;, &#34;add_&#34; to update target
                # params, as opposed to &#34;mul&#34; and &#34;add&#34;, which would make new tensors.
                p_targ.data.mul_(polyak)
                p_targ.data.add_((1 - polyak) * p.data)

    def get_action(o, deterministic=False):
        return ac.act(torch.as_tensor(o, dtype=torch.float32), 
                      deterministic)

    def test_agent():
        for j in range(num_test_episodes):
            if Teacher: Teacher.set_test_env_params(test_env)
            o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0
            while not(d or (ep_len == max_ep_len)):
                # Take deterministic actions at test time 
                o, r, d, _ = test_env.step(get_action(o, True))
                ep_ret += r
                ep_len += 1
            logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)
            if Teacher: Teacher.record_test_episode(ep_ret, ep_len)

    # Prepare for interaction with environment
    total_steps = steps_per_epoch * epochs
    start_time = time.time()
    o, ep_ret, ep_len = env.reset(), 0, 0
    Teacher.record_train_task_initial_state(o)

    # Main loop: collect experience in env and update/log each epoch
    for t in range(total_steps):
        
        # Until start_steps have elapsed, randomly sample actions
        # from a uniform distribution for better exploration. Afterwards, 
        # use the learned policy. 
        if t &gt; start_steps:
            a = get_action(o)
        else:
            a = env.action_space.sample()

        # Step the env
        o2, r, d, infos = env.step(a)
        ep_ret += r
        ep_len += 1
        Teacher.record_train_step(o, a, r, o2, d)

        # Ignore the &#34;done&#34; signal if it comes from hitting the time
        # horizon (that is, when it&#39;s an artificial terminal signal
        # that isn&#39;t based on the agent&#39;s state)
        d = False if ep_len==max_ep_len else d

        # Store experience to replay buffer
        replay_buffer.store(o, a, r, o2, d)

        # Super critical, easy to overlook step: make sure to update 
        # most recent observation!
        o = o2

        # End of trajectory handling
        if d or (ep_len == max_ep_len):
            logger.store(EpRet=ep_ret, EpLen=ep_len)
            if Teacher:
                success = False if &#39;success&#39; not in infos else infos[&#34;success&#34;]
                Teacher.record_train_episode(ep_ret, ep_len, success)
                params = Teacher.set_env_params(env)
            o, ep_ret, ep_len = env.reset(), 0, 0
            Teacher.record_train_task_initial_state(o)

        # Update handling
        if t &gt;= update_after and t % update_every == 0:
            #for j in range(update_every):
            batch = replay_buffer.sample_batch(batch_size)
            update(data=batch)

        # End of epoch handling
        if (t+1) % steps_per_epoch == 0:
            epoch = (t+1) // steps_per_epoch

            # Save model
            if epoch % save_freq == 0:  # or (epoch == epochs):
                if half_save and epoch == epochs / 2:
                    logger.save_state({&#39;env&#39;: env}, itr=epoch)
                else:
                    logger.save_state({&#39;env&#39;: env}, None)  # itr=epoch)

            # Test the performance of the deterministic version of the agent.
            test_agent()

            # Log info about epoch
            logger.log_tabular(&#39;Epoch&#39;, epoch)
            logger.log_tabular(&#39;EpRet&#39;, with_min_and_max=True)
            logger.log_tabular(&#39;TestEpRet&#39;, with_min_and_max=True)
            logger.log_tabular(&#39;EpLen&#39;, average_only=True)
            logger.log_tabular(&#39;TestEpLen&#39;, average_only=True)
            logger.log_tabular(&#39;TotalEnvInteracts&#39;, t)
            logger.log_tabular(&#39;Q1Vals&#39;, with_min_and_max=True)
            logger.log_tabular(&#39;Q2Vals&#39;, with_min_and_max=True)
            logger.log_tabular(&#39;LogPi&#39;, with_min_and_max=True)
            logger.log_tabular(&#39;LossPi&#39;, average_only=True)
            logger.log_tabular(&#39;LossQ&#39;, average_only=True)
            logger.log_tabular(&#39;Time&#39;, time.time()-start_time)
            logger.dump_tabular()

            # Pickle parameterized env data
            # print(logger.output_dir+&#39;/env_params_save.pkl&#39;)
            if Teacher: Teacher.dump(logger.output_dir + &#39;/env_params_save.pkl&#39;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="TeachMyAgent.students.spinup.algos.pytorch.sac_v02.sac.ReplayBuffer"><code class="flex name class">
<span>class <span class="ident">ReplayBuffer</span></span>
<span>(</span><span>obs_dim, act_dim, size)</span>
</code></dt>
<dd>
<div class="desc"><p>A simple FIFO experience replay buffer for SAC agents.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ReplayBuffer:
    &#34;&#34;&#34;
    A simple FIFO experience replay buffer for SAC agents.
    &#34;&#34;&#34;

    def __init__(self, obs_dim, act_dim, size):
        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)
        self.obs2_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)
        self.act_buf = np.zeros(core.combined_shape(size, act_dim), dtype=np.float32)
        self.rew_buf = np.zeros(size, dtype=np.float32)
        self.done_buf = np.zeros(size, dtype=np.float32)
        self.ptr, self.size, self.max_size = 0, 0, size

    def store(self, obs, act, rew, next_obs, done):
        self.obs_buf[self.ptr] = obs
        self.obs2_buf[self.ptr] = next_obs
        self.act_buf[self.ptr] = act
        self.rew_buf[self.ptr] = rew
        self.done_buf[self.ptr] = done
        self.ptr = (self.ptr+1) % self.max_size
        self.size = min(self.size+1, self.max_size)

    def sample_batch(self, batch_size=32):
        idxs = np.random.randint(0, self.size, size=batch_size)
        batch = dict(obs=self.obs_buf[idxs],
                     obs2=self.obs2_buf[idxs],
                     act=self.act_buf[idxs],
                     rew=self.rew_buf[idxs],
                     done=self.done_buf[idxs])
        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.spinup.algos.pytorch.sac_v02.sac.ReplayBuffer.sample_batch"><code class="name flex">
<span>def <span class="ident">sample_batch</span></span>(<span>self, batch_size=32)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_batch(self, batch_size=32):
    idxs = np.random.randint(0, self.size, size=batch_size)
    batch = dict(obs=self.obs_buf[idxs],
                 obs2=self.obs2_buf[idxs],
                 act=self.act_buf[idxs],
                 rew=self.rew_buf[idxs],
                 done=self.done_buf[idxs])
    return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.spinup.algos.pytorch.sac_v02.sac.ReplayBuffer.store"><code class="name flex">
<span>def <span class="ident">store</span></span>(<span>self, obs, act, rew, next_obs, done)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def store(self, obs, act, rew, next_obs, done):
    self.obs_buf[self.ptr] = obs
    self.obs2_buf[self.ptr] = next_obs
    self.act_buf[self.ptr] = act
    self.rew_buf[self.ptr] = rew
    self.done_buf[self.ptr] = done
    self.ptr = (self.ptr+1) % self.max_size
    self.size = min(self.size+1, self.max_size)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<a href="http://developmentalsystems.org/TeachMyAgent/">
<img src="https://github.com/flowersteam/TeachMyAgent/blob/gh-pages/images/home/head_image.png?raw=true" style="display: block; margin: 1em auto">
</a>
<a href="http://developmentalsystems.org/TeachMyAgent/">Website</a>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="TeachMyAgent.students.spinup.algos.pytorch.sac_v02" href="index.html">TeachMyAgent.students.spinup.algos.pytorch.sac_v02</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="TeachMyAgent.students.spinup.algos.pytorch.sac_v02.sac.sac" href="#TeachMyAgent.students.spinup.algos.pytorch.sac_v02.sac.sac">sac</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="TeachMyAgent.students.spinup.algos.pytorch.sac_v02.sac.ReplayBuffer" href="#TeachMyAgent.students.spinup.algos.pytorch.sac_v02.sac.ReplayBuffer">ReplayBuffer</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.students.spinup.algos.pytorch.sac_v02.sac.ReplayBuffer.sample_batch" href="#TeachMyAgent.students.spinup.algos.pytorch.sac_v02.sac.ReplayBuffer.sample_batch">sample_batch</a></code></li>
<li><code><a title="TeachMyAgent.students.spinup.algos.pytorch.sac_v02.sac.ReplayBuffer.store" href="#TeachMyAgent.students.spinup.algos.pytorch.sac_v02.sac.ReplayBuffer.store">store</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>