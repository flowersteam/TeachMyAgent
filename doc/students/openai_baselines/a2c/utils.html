<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>TeachMyAgent.students.openai_baselines.a2c.utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="icon" href="../images/favicon-96x96.png" />
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>TeachMyAgent.students.openai_baselines.a2c.utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import numpy as np
import tensorflow as tf
from collections import deque

def sample(logits):
    noise = tf.random_uniform(tf.shape(logits))
    return tf.argmax(logits - tf.log(-tf.log(noise)), 1)

def cat_entropy(logits):
    a0 = logits - tf.reduce_max(logits, 1, keepdims=True)
    ea0 = tf.exp(a0)
    z0 = tf.reduce_sum(ea0, 1, keepdims=True)
    p0 = ea0 / z0
    return tf.reduce_sum(p0 * (tf.log(z0) - a0), 1)

def cat_entropy_softmax(p0):
    return - tf.reduce_sum(p0 * tf.log(p0 + 1e-6), axis = 1)

def ortho_init(scale=1.0):
    def _ortho_init(shape, dtype, partition_info=None):
        #lasagne ortho init for tf
        shape = tuple(shape)
        if len(shape) == 2:
            flat_shape = shape
        elif len(shape) == 4: # assumes NHWC
            flat_shape = (np.prod(shape[:-1]), shape[-1])
        else:
            raise NotImplementedError
        a = np.random.normal(0.0, 1.0, flat_shape)
        u, _, v = np.linalg.svd(a, full_matrices=False)
        q = u if u.shape == flat_shape else v # pick the one with the correct shape
        q = q.reshape(shape)
        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)
    return _ortho_init

def conv(x, scope, *, nf, rf, stride, pad=&#39;VALID&#39;, init_scale=1.0, data_format=&#39;NHWC&#39;, one_dim_bias=False):
    if data_format == &#39;NHWC&#39;:
        channel_ax = 3
        strides = [1, stride, stride, 1]
        bshape = [1, 1, 1, nf]
    elif data_format == &#39;NCHW&#39;:
        channel_ax = 1
        strides = [1, 1, stride, stride]
        bshape = [1, nf, 1, 1]
    else:
        raise NotImplementedError
    bias_var_shape = [nf] if one_dim_bias else [1, nf, 1, 1]
    nin = x.get_shape()[channel_ax].value
    wshape = [rf, rf, nin, nf]
    with tf.variable_scope(scope):
        w = tf.get_variable(&#34;w&#34;, wshape, initializer=ortho_init(init_scale))
        b = tf.get_variable(&#34;b&#34;, bias_var_shape, initializer=tf.constant_initializer(0.0))
        if not one_dim_bias and data_format == &#39;NHWC&#39;:
            b = tf.reshape(b, bshape)
        return tf.nn.conv2d(x, w, strides=strides, padding=pad, data_format=data_format) + b

def fc(x, scope, nh, *, init_scale=1.0, init_bias=0.0):
    with tf.variable_scope(scope):
        nin = x.get_shape()[1].value
        w = tf.get_variable(&#34;w&#34;, [nin, nh], initializer=ortho_init(init_scale))
        b = tf.get_variable(&#34;b&#34;, [nh], initializer=tf.constant_initializer(init_bias))
        return tf.matmul(x, w)+b

def batch_to_seq(h, nbatch, nsteps, flat=False):
    if flat:
        h = tf.reshape(h, [nbatch, nsteps])
    else:
        h = tf.reshape(h, [nbatch, nsteps, -1])
    return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]

def seq_to_batch(h, flat = False):
    shape = h[0].get_shape().as_list()
    if not flat:
        assert(len(shape) &gt; 1)
        nh = h[0].get_shape()[-1].value
        return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])
    else:
        return tf.reshape(tf.stack(values=h, axis=1), [-1])

def lstm(xs, ms, s, scope, nh, init_scale=1.0):
    nbatch, nin = [v.value for v in xs[0].get_shape()]
    with tf.variable_scope(scope):
        wx = tf.get_variable(&#34;wx&#34;, [nin, nh*4], initializer=ortho_init(init_scale))
        wh = tf.get_variable(&#34;wh&#34;, [nh, nh*4], initializer=ortho_init(init_scale))
        b = tf.get_variable(&#34;b&#34;, [nh*4], initializer=tf.constant_initializer(0.0))

    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)
    for idx, (x, m) in enumerate(zip(xs, ms)):
        c = c*(1-m)
        h = h*(1-m)
        z = tf.matmul(x, wx) + tf.matmul(h, wh) + b
        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)
        i = tf.nn.sigmoid(i)
        f = tf.nn.sigmoid(f)
        o = tf.nn.sigmoid(o)
        u = tf.tanh(u)
        c = f*c + i*u
        h = o*tf.tanh(c)
        xs[idx] = h
    s = tf.concat(axis=1, values=[c, h])
    return xs, s

def _ln(x, g, b, e=1e-5, axes=[1]):
    u, s = tf.nn.moments(x, axes=axes, keep_dims=True)
    x = (x-u)/tf.sqrt(s+e)
    x = x*g+b
    return x

def lnlstm(xs, ms, s, scope, nh, init_scale=1.0):
    nbatch, nin = [v.value for v in xs[0].get_shape()]
    with tf.variable_scope(scope):
        wx = tf.get_variable(&#34;wx&#34;, [nin, nh*4], initializer=ortho_init(init_scale))
        gx = tf.get_variable(&#34;gx&#34;, [nh*4], initializer=tf.constant_initializer(1.0))
        bx = tf.get_variable(&#34;bx&#34;, [nh*4], initializer=tf.constant_initializer(0.0))

        wh = tf.get_variable(&#34;wh&#34;, [nh, nh*4], initializer=ortho_init(init_scale))
        gh = tf.get_variable(&#34;gh&#34;, [nh*4], initializer=tf.constant_initializer(1.0))
        bh = tf.get_variable(&#34;bh&#34;, [nh*4], initializer=tf.constant_initializer(0.0))

        b = tf.get_variable(&#34;b&#34;, [nh*4], initializer=tf.constant_initializer(0.0))

        gc = tf.get_variable(&#34;gc&#34;, [nh], initializer=tf.constant_initializer(1.0))
        bc = tf.get_variable(&#34;bc&#34;, [nh], initializer=tf.constant_initializer(0.0))

    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)
    for idx, (x, m) in enumerate(zip(xs, ms)):
        c = c*(1-m)
        h = h*(1-m)
        z = _ln(tf.matmul(x, wx), gx, bx) + _ln(tf.matmul(h, wh), gh, bh) + b
        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)
        i = tf.nn.sigmoid(i)
        f = tf.nn.sigmoid(f)
        o = tf.nn.sigmoid(o)
        u = tf.tanh(u)
        c = f*c + i*u
        h = o*tf.tanh(_ln(c, gc, bc))
        xs[idx] = h
    s = tf.concat(axis=1, values=[c, h])
    return xs, s

def conv_to_fc(x):
    nh = np.prod([v.value for v in x.get_shape()[1:]])
    x = tf.reshape(x, [-1, nh])
    return x

def discount_with_dones(rewards, dones, gamma):
    discounted = []
    r = 0
    for reward, done in zip(rewards[::-1], dones[::-1]):
        r = reward + gamma*r*(1.-done) # fixed off by one bug
        discounted.append(r)
    return discounted[::-1]

def find_trainable_variables(key):
    return tf.trainable_variables(key)

def make_path(f):
    return os.makedirs(f, exist_ok=True)

def constant(p):
    return 1

def linear(p):
    return 1-p

def middle_drop(p):
    eps = 0.75
    if 1-p&lt;eps:
        return eps*0.1
    return 1-p

def double_linear_con(p):
    p *= 2
    eps = 0.125
    if 1-p&lt;eps:
        return eps
    return 1-p

def double_middle_drop(p):
    eps1 = 0.75
    eps2 = 0.25
    if 1-p&lt;eps1:
        if 1-p&lt;eps2:
            return eps2*0.5
        return eps1*0.1
    return 1-p

schedules = {
    &#39;linear&#39;:linear,
    &#39;constant&#39;:constant,
    &#39;double_linear_con&#39;: double_linear_con,
    &#39;middle_drop&#39;: middle_drop,
    &#39;double_middle_drop&#39;: double_middle_drop
}

class Scheduler(object):

    def __init__(self, v, nvalues, schedule):
        self.n = 0.
        self.v = v
        self.nvalues = nvalues
        self.schedule = schedules[schedule]

    def value(self):
        current_value = self.v*self.schedule(self.n/self.nvalues)
        self.n += 1.
        return current_value

    def value_steps(self, steps):
        return self.v*self.schedule(steps/self.nvalues)


class EpisodeStats:
    def __init__(self, nsteps, nenvs):
        self.episode_rewards = []
        for i in range(nenvs):
            self.episode_rewards.append([])
        self.lenbuffer = deque(maxlen=40)  # rolling buffer for episode lengths
        self.rewbuffer = deque(maxlen=40)  # rolling buffer for episode rewards
        self.nsteps = nsteps
        self.nenvs = nenvs

    def feed(self, rewards, masks):
        rewards = np.reshape(rewards, [self.nenvs, self.nsteps])
        masks = np.reshape(masks, [self.nenvs, self.nsteps])
        for i in range(0, self.nenvs):
            for j in range(0, self.nsteps):
                self.episode_rewards[i].append(rewards[i][j])
                if masks[i][j]:
                    l = len(self.episode_rewards[i])
                    s = sum(self.episode_rewards[i])
                    self.lenbuffer.append(l)
                    self.rewbuffer.append(s)
                    self.episode_rewards[i] = []

    def mean_length(self):
        if self.lenbuffer:
            return np.mean(self.lenbuffer)
        else:
            return 0  # on the first params dump, no episodes are finished

    def mean_reward(self):
        if self.rewbuffer:
            return np.mean(self.rewbuffer)
        else:
            return 0


# For ACER
def get_by_index(x, idx):
    assert(len(x.get_shape()) == 2)
    assert(len(idx.get_shape()) == 1)
    idx_flattened = tf.range(0, x.shape[0]) * x.shape[1] + idx
    y = tf.gather(tf.reshape(x, [-1]),  # flatten input
                  idx_flattened)  # use flattened indices
    return y

def check_shape(ts,shapes):
    i = 0
    for (t,shape) in zip(ts,shapes):
        assert t.get_shape().as_list()==shape, &#34;id &#34; + str(i) + &#34; shape &#34; + str(t.get_shape()) + str(shape)
        i += 1

def avg_norm(t):
    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(t), axis=-1)))

def gradient_add(g1, g2, param):
    print([g1, g2, param.name])
    assert (not (g1 is None and g2 is None)), param.name
    if g1 is None:
        return g2
    elif g2 is None:
        return g1
    else:
        return g1 + g2

def q_explained_variance(qpred, q):
    _, vary = tf.nn.moments(q, axes=[0, 1])
    _, varpred = tf.nn.moments(q - qpred, axes=[0, 1])
    check_shape([vary, varpred], [[]] * 2)
    return 1.0 - (varpred / vary)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.avg_norm"><code class="name flex">
<span>def <span class="ident">avg_norm</span></span>(<span>t)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def avg_norm(t):
    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(t), axis=-1)))</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.batch_to_seq"><code class="name flex">
<span>def <span class="ident">batch_to_seq</span></span>(<span>h, nbatch, nsteps, flat=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_to_seq(h, nbatch, nsteps, flat=False):
    if flat:
        h = tf.reshape(h, [nbatch, nsteps])
    else:
        h = tf.reshape(h, [nbatch, nsteps, -1])
    return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.cat_entropy"><code class="name flex">
<span>def <span class="ident">cat_entropy</span></span>(<span>logits)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cat_entropy(logits):
    a0 = logits - tf.reduce_max(logits, 1, keepdims=True)
    ea0 = tf.exp(a0)
    z0 = tf.reduce_sum(ea0, 1, keepdims=True)
    p0 = ea0 / z0
    return tf.reduce_sum(p0 * (tf.log(z0) - a0), 1)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.cat_entropy_softmax"><code class="name flex">
<span>def <span class="ident">cat_entropy_softmax</span></span>(<span>p0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cat_entropy_softmax(p0):
    return - tf.reduce_sum(p0 * tf.log(p0 + 1e-6), axis = 1)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.check_shape"><code class="name flex">
<span>def <span class="ident">check_shape</span></span>(<span>ts, shapes)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_shape(ts,shapes):
    i = 0
    for (t,shape) in zip(ts,shapes):
        assert t.get_shape().as_list()==shape, &#34;id &#34; + str(i) + &#34; shape &#34; + str(t.get_shape()) + str(shape)
        i += 1</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.constant"><code class="name flex">
<span>def <span class="ident">constant</span></span>(<span>p)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def constant(p):
    return 1</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.conv"><code class="name flex">
<span>def <span class="ident">conv</span></span>(<span>x, scope, *, nf, rf, stride, pad='VALID', init_scale=1.0, data_format='NHWC', one_dim_bias=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conv(x, scope, *, nf, rf, stride, pad=&#39;VALID&#39;, init_scale=1.0, data_format=&#39;NHWC&#39;, one_dim_bias=False):
    if data_format == &#39;NHWC&#39;:
        channel_ax = 3
        strides = [1, stride, stride, 1]
        bshape = [1, 1, 1, nf]
    elif data_format == &#39;NCHW&#39;:
        channel_ax = 1
        strides = [1, 1, stride, stride]
        bshape = [1, nf, 1, 1]
    else:
        raise NotImplementedError
    bias_var_shape = [nf] if one_dim_bias else [1, nf, 1, 1]
    nin = x.get_shape()[channel_ax].value
    wshape = [rf, rf, nin, nf]
    with tf.variable_scope(scope):
        w = tf.get_variable(&#34;w&#34;, wshape, initializer=ortho_init(init_scale))
        b = tf.get_variable(&#34;b&#34;, bias_var_shape, initializer=tf.constant_initializer(0.0))
        if not one_dim_bias and data_format == &#39;NHWC&#39;:
            b = tf.reshape(b, bshape)
        return tf.nn.conv2d(x, w, strides=strides, padding=pad, data_format=data_format) + b</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.conv_to_fc"><code class="name flex">
<span>def <span class="ident">conv_to_fc</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conv_to_fc(x):
    nh = np.prod([v.value for v in x.get_shape()[1:]])
    x = tf.reshape(x, [-1, nh])
    return x</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.discount_with_dones"><code class="name flex">
<span>def <span class="ident">discount_with_dones</span></span>(<span>rewards, dones, gamma)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def discount_with_dones(rewards, dones, gamma):
    discounted = []
    r = 0
    for reward, done in zip(rewards[::-1], dones[::-1]):
        r = reward + gamma*r*(1.-done) # fixed off by one bug
        discounted.append(r)
    return discounted[::-1]</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.double_linear_con"><code class="name flex">
<span>def <span class="ident">double_linear_con</span></span>(<span>p)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def double_linear_con(p):
    p *= 2
    eps = 0.125
    if 1-p&lt;eps:
        return eps
    return 1-p</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.double_middle_drop"><code class="name flex">
<span>def <span class="ident">double_middle_drop</span></span>(<span>p)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def double_middle_drop(p):
    eps1 = 0.75
    eps2 = 0.25
    if 1-p&lt;eps1:
        if 1-p&lt;eps2:
            return eps2*0.5
        return eps1*0.1
    return 1-p</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.fc"><code class="name flex">
<span>def <span class="ident">fc</span></span>(<span>x, scope, nh, *, init_scale=1.0, init_bias=0.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fc(x, scope, nh, *, init_scale=1.0, init_bias=0.0):
    with tf.variable_scope(scope):
        nin = x.get_shape()[1].value
        w = tf.get_variable(&#34;w&#34;, [nin, nh], initializer=ortho_init(init_scale))
        b = tf.get_variable(&#34;b&#34;, [nh], initializer=tf.constant_initializer(init_bias))
        return tf.matmul(x, w)+b</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.find_trainable_variables"><code class="name flex">
<span>def <span class="ident">find_trainable_variables</span></span>(<span>key)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_trainable_variables(key):
    return tf.trainable_variables(key)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.get_by_index"><code class="name flex">
<span>def <span class="ident">get_by_index</span></span>(<span>x, idx)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_by_index(x, idx):
    assert(len(x.get_shape()) == 2)
    assert(len(idx.get_shape()) == 1)
    idx_flattened = tf.range(0, x.shape[0]) * x.shape[1] + idx
    y = tf.gather(tf.reshape(x, [-1]),  # flatten input
                  idx_flattened)  # use flattened indices
    return y</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.gradient_add"><code class="name flex">
<span>def <span class="ident">gradient_add</span></span>(<span>g1, g2, param)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gradient_add(g1, g2, param):
    print([g1, g2, param.name])
    assert (not (g1 is None and g2 is None)), param.name
    if g1 is None:
        return g2
    elif g2 is None:
        return g1
    else:
        return g1 + g2</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.linear"><code class="name flex">
<span>def <span class="ident">linear</span></span>(<span>p)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def linear(p):
    return 1-p</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.lnlstm"><code class="name flex">
<span>def <span class="ident">lnlstm</span></span>(<span>xs, ms, s, scope, nh, init_scale=1.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lnlstm(xs, ms, s, scope, nh, init_scale=1.0):
    nbatch, nin = [v.value for v in xs[0].get_shape()]
    with tf.variable_scope(scope):
        wx = tf.get_variable(&#34;wx&#34;, [nin, nh*4], initializer=ortho_init(init_scale))
        gx = tf.get_variable(&#34;gx&#34;, [nh*4], initializer=tf.constant_initializer(1.0))
        bx = tf.get_variable(&#34;bx&#34;, [nh*4], initializer=tf.constant_initializer(0.0))

        wh = tf.get_variable(&#34;wh&#34;, [nh, nh*4], initializer=ortho_init(init_scale))
        gh = tf.get_variable(&#34;gh&#34;, [nh*4], initializer=tf.constant_initializer(1.0))
        bh = tf.get_variable(&#34;bh&#34;, [nh*4], initializer=tf.constant_initializer(0.0))

        b = tf.get_variable(&#34;b&#34;, [nh*4], initializer=tf.constant_initializer(0.0))

        gc = tf.get_variable(&#34;gc&#34;, [nh], initializer=tf.constant_initializer(1.0))
        bc = tf.get_variable(&#34;bc&#34;, [nh], initializer=tf.constant_initializer(0.0))

    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)
    for idx, (x, m) in enumerate(zip(xs, ms)):
        c = c*(1-m)
        h = h*(1-m)
        z = _ln(tf.matmul(x, wx), gx, bx) + _ln(tf.matmul(h, wh), gh, bh) + b
        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)
        i = tf.nn.sigmoid(i)
        f = tf.nn.sigmoid(f)
        o = tf.nn.sigmoid(o)
        u = tf.tanh(u)
        c = f*c + i*u
        h = o*tf.tanh(_ln(c, gc, bc))
        xs[idx] = h
    s = tf.concat(axis=1, values=[c, h])
    return xs, s</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.lstm"><code class="name flex">
<span>def <span class="ident">lstm</span></span>(<span>xs, ms, s, scope, nh, init_scale=1.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lstm(xs, ms, s, scope, nh, init_scale=1.0):
    nbatch, nin = [v.value for v in xs[0].get_shape()]
    with tf.variable_scope(scope):
        wx = tf.get_variable(&#34;wx&#34;, [nin, nh*4], initializer=ortho_init(init_scale))
        wh = tf.get_variable(&#34;wh&#34;, [nh, nh*4], initializer=ortho_init(init_scale))
        b = tf.get_variable(&#34;b&#34;, [nh*4], initializer=tf.constant_initializer(0.0))

    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)
    for idx, (x, m) in enumerate(zip(xs, ms)):
        c = c*(1-m)
        h = h*(1-m)
        z = tf.matmul(x, wx) + tf.matmul(h, wh) + b
        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)
        i = tf.nn.sigmoid(i)
        f = tf.nn.sigmoid(f)
        o = tf.nn.sigmoid(o)
        u = tf.tanh(u)
        c = f*c + i*u
        h = o*tf.tanh(c)
        xs[idx] = h
    s = tf.concat(axis=1, values=[c, h])
    return xs, s</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.make_path"><code class="name flex">
<span>def <span class="ident">make_path</span></span>(<span>f)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_path(f):
    return os.makedirs(f, exist_ok=True)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.middle_drop"><code class="name flex">
<span>def <span class="ident">middle_drop</span></span>(<span>p)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def middle_drop(p):
    eps = 0.75
    if 1-p&lt;eps:
        return eps*0.1
    return 1-p</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.ortho_init"><code class="name flex">
<span>def <span class="ident">ortho_init</span></span>(<span>scale=1.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ortho_init(scale=1.0):
    def _ortho_init(shape, dtype, partition_info=None):
        #lasagne ortho init for tf
        shape = tuple(shape)
        if len(shape) == 2:
            flat_shape = shape
        elif len(shape) == 4: # assumes NHWC
            flat_shape = (np.prod(shape[:-1]), shape[-1])
        else:
            raise NotImplementedError
        a = np.random.normal(0.0, 1.0, flat_shape)
        u, _, v = np.linalg.svd(a, full_matrices=False)
        q = u if u.shape == flat_shape else v # pick the one with the correct shape
        q = q.reshape(shape)
        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)
    return _ortho_init</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.q_explained_variance"><code class="name flex">
<span>def <span class="ident">q_explained_variance</span></span>(<span>qpred, q)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def q_explained_variance(qpred, q):
    _, vary = tf.nn.moments(q, axes=[0, 1])
    _, varpred = tf.nn.moments(q - qpred, axes=[0, 1])
    check_shape([vary, varpred], [[]] * 2)
    return 1.0 - (varpred / vary)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.sample"><code class="name flex">
<span>def <span class="ident">sample</span></span>(<span>logits)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample(logits):
    noise = tf.random_uniform(tf.shape(logits))
    return tf.argmax(logits - tf.log(-tf.log(noise)), 1)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.seq_to_batch"><code class="name flex">
<span>def <span class="ident">seq_to_batch</span></span>(<span>h, flat=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def seq_to_batch(h, flat = False):
    shape = h[0].get_shape().as_list()
    if not flat:
        assert(len(shape) &gt; 1)
        nh = h[0].get_shape()[-1].value
        return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])
    else:
        return tf.reshape(tf.stack(values=h, axis=1), [-1])</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.EpisodeStats"><code class="flex name class">
<span>class <span class="ident">EpisodeStats</span></span>
<span>(</span><span>nsteps, nenvs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EpisodeStats:
    def __init__(self, nsteps, nenvs):
        self.episode_rewards = []
        for i in range(nenvs):
            self.episode_rewards.append([])
        self.lenbuffer = deque(maxlen=40)  # rolling buffer for episode lengths
        self.rewbuffer = deque(maxlen=40)  # rolling buffer for episode rewards
        self.nsteps = nsteps
        self.nenvs = nenvs

    def feed(self, rewards, masks):
        rewards = np.reshape(rewards, [self.nenvs, self.nsteps])
        masks = np.reshape(masks, [self.nenvs, self.nsteps])
        for i in range(0, self.nenvs):
            for j in range(0, self.nsteps):
                self.episode_rewards[i].append(rewards[i][j])
                if masks[i][j]:
                    l = len(self.episode_rewards[i])
                    s = sum(self.episode_rewards[i])
                    self.lenbuffer.append(l)
                    self.rewbuffer.append(s)
                    self.episode_rewards[i] = []

    def mean_length(self):
        if self.lenbuffer:
            return np.mean(self.lenbuffer)
        else:
            return 0  # on the first params dump, no episodes are finished

    def mean_reward(self):
        if self.rewbuffer:
            return np.mean(self.rewbuffer)
        else:
            return 0</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.EpisodeStats.feed"><code class="name flex">
<span>def <span class="ident">feed</span></span>(<span>self, rewards, masks)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def feed(self, rewards, masks):
    rewards = np.reshape(rewards, [self.nenvs, self.nsteps])
    masks = np.reshape(masks, [self.nenvs, self.nsteps])
    for i in range(0, self.nenvs):
        for j in range(0, self.nsteps):
            self.episode_rewards[i].append(rewards[i][j])
            if masks[i][j]:
                l = len(self.episode_rewards[i])
                s = sum(self.episode_rewards[i])
                self.lenbuffer.append(l)
                self.rewbuffer.append(s)
                self.episode_rewards[i] = []</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.EpisodeStats.mean_length"><code class="name flex">
<span>def <span class="ident">mean_length</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean_length(self):
    if self.lenbuffer:
        return np.mean(self.lenbuffer)
    else:
        return 0  # on the first params dump, no episodes are finished</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.EpisodeStats.mean_reward"><code class="name flex">
<span>def <span class="ident">mean_reward</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean_reward(self):
    if self.rewbuffer:
        return np.mean(self.rewbuffer)
    else:
        return 0</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.Scheduler"><code class="flex name class">
<span>class <span class="ident">Scheduler</span></span>
<span>(</span><span>v, nvalues, schedule)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Scheduler(object):

    def __init__(self, v, nvalues, schedule):
        self.n = 0.
        self.v = v
        self.nvalues = nvalues
        self.schedule = schedules[schedule]

    def value(self):
        current_value = self.v*self.schedule(self.n/self.nvalues)
        self.n += 1.
        return current_value

    def value_steps(self, steps):
        return self.v*self.schedule(steps/self.nvalues)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.Scheduler.value"><code class="name flex">
<span>def <span class="ident">value</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def value(self):
    current_value = self.v*self.schedule(self.n/self.nvalues)
    self.n += 1.
    return current_value</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.a2c.utils.Scheduler.value_steps"><code class="name flex">
<span>def <span class="ident">value_steps</span></span>(<span>self, steps)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def value_steps(self, steps):
    return self.v*self.schedule(steps/self.nvalues)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<a href="http://developmentalsystems.org/TeachMyAgent/doc/">
<img src="../images/home/head_image.png" style="display: block; margin: 1em auto">
</a>
<a href="http://developmentalsystems.org/TeachMyAgent/doc/">Home</a> | <a href="http://developmentalsystems.org/TeachMyAgent/">Website</a>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c" href="index.html">TeachMyAgent.students.openai_baselines.a2c</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.avg_norm" href="#TeachMyAgent.students.openai_baselines.a2c.utils.avg_norm">avg_norm</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.batch_to_seq" href="#TeachMyAgent.students.openai_baselines.a2c.utils.batch_to_seq">batch_to_seq</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.cat_entropy" href="#TeachMyAgent.students.openai_baselines.a2c.utils.cat_entropy">cat_entropy</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.cat_entropy_softmax" href="#TeachMyAgent.students.openai_baselines.a2c.utils.cat_entropy_softmax">cat_entropy_softmax</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.check_shape" href="#TeachMyAgent.students.openai_baselines.a2c.utils.check_shape">check_shape</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.constant" href="#TeachMyAgent.students.openai_baselines.a2c.utils.constant">constant</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.conv" href="#TeachMyAgent.students.openai_baselines.a2c.utils.conv">conv</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.conv_to_fc" href="#TeachMyAgent.students.openai_baselines.a2c.utils.conv_to_fc">conv_to_fc</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.discount_with_dones" href="#TeachMyAgent.students.openai_baselines.a2c.utils.discount_with_dones">discount_with_dones</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.double_linear_con" href="#TeachMyAgent.students.openai_baselines.a2c.utils.double_linear_con">double_linear_con</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.double_middle_drop" href="#TeachMyAgent.students.openai_baselines.a2c.utils.double_middle_drop">double_middle_drop</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.fc" href="#TeachMyAgent.students.openai_baselines.a2c.utils.fc">fc</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.find_trainable_variables" href="#TeachMyAgent.students.openai_baselines.a2c.utils.find_trainable_variables">find_trainable_variables</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.get_by_index" href="#TeachMyAgent.students.openai_baselines.a2c.utils.get_by_index">get_by_index</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.gradient_add" href="#TeachMyAgent.students.openai_baselines.a2c.utils.gradient_add">gradient_add</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.linear" href="#TeachMyAgent.students.openai_baselines.a2c.utils.linear">linear</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.lnlstm" href="#TeachMyAgent.students.openai_baselines.a2c.utils.lnlstm">lnlstm</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.lstm" href="#TeachMyAgent.students.openai_baselines.a2c.utils.lstm">lstm</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.make_path" href="#TeachMyAgent.students.openai_baselines.a2c.utils.make_path">make_path</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.middle_drop" href="#TeachMyAgent.students.openai_baselines.a2c.utils.middle_drop">middle_drop</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.ortho_init" href="#TeachMyAgent.students.openai_baselines.a2c.utils.ortho_init">ortho_init</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.q_explained_variance" href="#TeachMyAgent.students.openai_baselines.a2c.utils.q_explained_variance">q_explained_variance</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.sample" href="#TeachMyAgent.students.openai_baselines.a2c.utils.sample">sample</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.seq_to_batch" href="#TeachMyAgent.students.openai_baselines.a2c.utils.seq_to_batch">seq_to_batch</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.EpisodeStats" href="#TeachMyAgent.students.openai_baselines.a2c.utils.EpisodeStats">EpisodeStats</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.EpisodeStats.feed" href="#TeachMyAgent.students.openai_baselines.a2c.utils.EpisodeStats.feed">feed</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.EpisodeStats.mean_length" href="#TeachMyAgent.students.openai_baselines.a2c.utils.EpisodeStats.mean_length">mean_length</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.EpisodeStats.mean_reward" href="#TeachMyAgent.students.openai_baselines.a2c.utils.EpisodeStats.mean_reward">mean_reward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.Scheduler" href="#TeachMyAgent.students.openai_baselines.a2c.utils.Scheduler">Scheduler</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.Scheduler.value" href="#TeachMyAgent.students.openai_baselines.a2c.utils.Scheduler.value">value</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.a2c.utils.Scheduler.value_steps" href="#TeachMyAgent.students.openai_baselines.a2c.utils.Scheduler.value_steps">value_steps</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>